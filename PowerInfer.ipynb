{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNexGVsy197vWBOu07ekaAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickylin1/PowerInfer/blob/main/PowerInfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPbY1Gna0Z7y",
        "outputId": "340a03fe-57cc-464c-e532-9feb6b6bad2f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PowerInfer'...\n",
            "remote: Enumerating objects: 8889, done.\u001b[K\n",
            "remote: Counting objects: 100% (672/672), done.\u001b[K\n",
            "remote: Compressing objects: 100% (392/392), done.\u001b[K\n",
            "remote: Total 8889 (delta 374), reused 534 (delta 270), pack-reused 8217\u001b[K\n",
            "Receiving objects: 100% (8889/8889), 12.20 MiB | 15.77 MiB/s, done.\n",
            "Resolving deltas: 100% (6188/6188), done.\n",
            "/content/PowerInfer\n",
            "Processing ./gguf-py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Processing ./powerinfer-py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: sentencepiece>=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers>=4.33.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 3)) (4.66.4)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from powerinfer==0.0.1->-r requirements.txt (line 5)) (2.3.1+cu121)\n",
            "Requirement already satisfied: cvxopt==1.3.2 in /usr/local/lib/python3.10/dist-packages (from powerinfer==0.0.1->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.33.2->-r requirements.txt (line 3)) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.33.2->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (12.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.2->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.2->-r requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.2->-r requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.2->-r requirements.txt (line 3)) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->powerinfer==0.0.1->-r requirements.txt (line 5)) (1.3.0)\n",
            "Building wheels for collected packages: gguf, powerinfer\n",
            "  Building wheel for gguf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gguf: filename=gguf-0.5.2-py3-none-any.whl size=23853 sha256=a968a8655f8a6c279268ff7c0a00cbe8df0044cb87fcfd71eb657e024c4d2814\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/4c/e0/9838075e25966db384770da8f910e7fffb89a229683f7eca0f\n",
            "  Building wheel for powerinfer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for powerinfer: filename=powerinfer-0.0.1-py3-none-any.whl size=4232 sha256=f3a825c3c3312f4a5d2f10b8c924bc59918f433e80a538b1cfbdc59a99832985\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/c6/23/f4a738ff75b5045bd50b1aff4c256fa5b8a5102ba0889786d7\n",
            "Successfully built gguf powerinfer\n",
            "Installing collected packages: gguf, powerinfer\n",
            "  Attempting uninstall: gguf\n",
            "    Found existing installation: gguf 0.5.2\n",
            "    Uninstalling gguf-0.5.2:\n",
            "      Successfully uninstalled gguf-0.5.2\n",
            "  Attempting uninstall: powerinfer\n",
            "    Found existing installation: powerinfer 0.0.1\n",
            "    Uninstalling powerinfer-0.0.1:\n",
            "      Successfully uninstalled powerinfer-0.0.1\n",
            "Successfully installed gguf-0.5.2 powerinfer-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SJTU-IPADS/PowerInfer\n",
        "%cd PowerInfer\n",
        "!pip install -r requirements.txt # install Python helpers' dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -S . -B build -DLLAMA_CUBLAS=ON\n",
        "!cmake --build build --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huc2BTTR0edT",
        "outputId": "71b06e6f-ae01-40f4-af35-ea4a78ed63da",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n",
            "-- cuBLAS found\n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Using CUDA architectures: 52;61;70\n",
            "GNU ld (GNU Binutils for Ubuntu) 2.38\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done (4.1s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/PowerInfer/build\n",
            "[  1%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_get_n_tasks\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml.c:2006:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Karray subscript 71 is above array bounds of ‘\u001b[01m\u001b[Kconst char *[70]\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Warray-bounds\u0007-Warray-bounds\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2006 |     return \u001b[01;35m\u001b[KGGML_OP_NAME[op]\u001b[m\u001b[K;\n",
            "      |            \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml.c:1586:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kwhile referencing ‘\u001b[01m\u001b[KGGML_OP_NAME\u001b[m\u001b[K’\n",
            " 1586 | static const char * \u001b[01;36m\u001b[KGGML_OP_NAME\u001b[m\u001b[K[GGML_OP_COUNT] = {\n",
            "      |                     \u001b[01;36m\u001b[K^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/include/stdio.h:894\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml.c:21\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kprintf\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kggml_graph_print\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/ggml.c:18011:9\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/stdio2.h:112:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K%16s\u001b[m\u001b[K’ directive argument is null [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat-overflow=\u0007-Wformat-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  112 |   return \u001b[01;35m\u001b[K__printf_chk (__USE_FORTIFY_LEVEL - 1, __fmt, __va_arg_pack ())\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[  2%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_axpy_q4_0_q8_0\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2457:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2457 |         __m256 by = _mm256_loadu_ps((const __m256 *)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar *)vy+i*128));\n",
            "      |                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2457:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2457 |         __m256 by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)((char *)vy+i*128)\u001b[m\u001b[K);\n",
            "      |                                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                     \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
            "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
            "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2460:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2460 |         _mm256_storeu_ps((__m256*)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vz + i*128), by);\n",
            "      |                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2460:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2460 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)((char*)vz + i*128)\u001b[m\u001b[K, by);\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
            "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
            "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2467:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2467 |         by = _mm256_loadu_ps((const __m256 *)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vy+i*128+32));\n",
            "      |                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2467:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2467 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)((char*)vy+i*128+32)\u001b[m\u001b[K);\n",
            "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
            "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
            "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2469:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2469 |         _mm256_storeu_ps((__m256*)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vz + i*128+32), by);\n",
            "      |                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2469:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2469 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)((char*)vz + i*128+32)\u001b[m\u001b[K, by);\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
            "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
            "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2479:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2479 |         by = _mm256_loadu_ps((const __m256 *)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vy+i*128+64));\n",
            "      |                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2479:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2479 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)((char*)vy+i*128+64)\u001b[m\u001b[K);\n",
            "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
            "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
            "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2482:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2482 |         _mm256_storeu_ps((__m256*)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vz + i*128+64), by);\n",
            "      |                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2482:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2482 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)((char*)vz + i*128+64)\u001b[m\u001b[K, by);\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
            "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
            "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2489:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2489 |         by = _mm256_loadu_ps((const __m256 *)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vy+i*128+96));\n",
            "      |                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2489:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2489 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)((char*)vy+i*128+96)\u001b[m\u001b[K);\n",
            "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
            "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
            "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2491:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2491 |         _mm256_storeu_ps((__m256*)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vz + i*128+96), by);\n",
            "      |                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2491:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2491 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)((char*)vz + i*128+96)\u001b[m\u001b[K, by);\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
            "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
            "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2435:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kacc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2435 |     __m256 \u001b[01;35m\u001b[Kacc\u001b[m\u001b[K = _mm256_setzero_ps();\n",
            "      |            \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "[  5%] \u001b[32mBuilding CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o\u001b[0m\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6717)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne0 = src->ne[0];\n",
            "                    ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6979)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"nrows_dst\"\u001b[0m was declared but never referenced\n",
            "      const int64_t nrows_dst = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7204)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7226)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"src1_dfloat\"\u001b[0m was declared but never referenced\n",
            "      const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i;\n",
            "                     ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7255)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7357)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
            "      int predict_idx = idx;\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8430)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne01 = src0->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8773)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
            "      bool all_on_device = (src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT) &&\n",
            "           ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4421)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4549)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4484)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
            "      short *d = (short *)((char *)vx + ncols * gpu_row * 2);\n",
            "             ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(579)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
            "                   void sigmoid_f32(const float * x, float * dst, const int k) {\n",
            "                        ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(5353)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"dequantize_mul_mat_vec_q4_0_cuda_sparse\"\u001b[0m was declared but never referenced\n",
            "  static void dequantize_mul_mat_vec_q4_0_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n",
            "              ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6717)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne0 = src->ne[0];\n",
            "                    ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6979)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"nrows_dst\"\u001b[0m was declared but never referenced\n",
            "      const int64_t nrows_dst = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7204)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7226)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"src1_dfloat\"\u001b[0m was declared but never referenced\n",
            "      const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i;\n",
            "                     ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7255)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7357)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
            "      int predict_idx = idx;\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8430)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne01 = src0->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8773)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
            "      bool all_on_device = (src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT) &&\n",
            "           ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4421)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4549)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4484)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
            "      short *d = (short *)((char *)vx + ncols * gpu_row * 2);\n",
            "             ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(579)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
            "                   void sigmoid_f32(const float * x, float * dst, const int k) {\n",
            "                        ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(5353)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"dequantize_mul_mat_vec_q4_0_cuda_sparse\"\u001b[0m was declared but never referenced\n",
            "  static void dequantize_mul_mat_vec_q4_0_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n",
            "              ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6717)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne0 = src->ne[0];\n",
            "                    ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6979)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"nrows_dst\"\u001b[0m was declared but never referenced\n",
            "      const int64_t nrows_dst = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7204)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7226)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"src1_dfloat\"\u001b[0m was declared but never referenced\n",
            "      const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i;\n",
            "                     ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7255)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7357)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
            "      int predict_idx = idx;\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8430)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne01 = src0->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8773)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
            "      bool all_on_device = (src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT) &&\n",
            "           ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4421)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4549)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4484)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
            "      short *d = (short *)((char *)vx + ncols * gpu_row * 2);\n",
            "             ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(579)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
            "                   void sigmoid_f32(const float * x, float * dst, const int k) {\n",
            "                        ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(5353)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"dequantize_mul_mat_vec_q4_0_cuda_sparse\"\u001b[0m was declared but never referenced\n",
            "  static void dequantize_mul_mat_vec_q4_0_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n",
            "              ^\n",
            "\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_batch_sparse(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:6962:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ksrc1_ddq_i\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 6961 |     const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i\u001b[01;35m\u001b[K, const float * src1_ddf_i,\u001b[m\u001b[K\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            " 6962 | \u001b[01;35m\u001b[K    const \u001b[m\u001b[Kchar * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n",
            "      | \u001b[01;35m\u001b[K^\u001b[m\u001b[K   \u001b[01;35m\u001b[K~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:6963:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ksrc1_padded_row_size\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 6962 |     const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const in\u001b[01;35m\u001b[Kt64_t src1_ncols,\u001b[m\u001b[K\n",
            "      |                                                                                                       \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            " 6963 | \u001b[01;35m\u001b[K    const int64_t sr\u001b[m\u001b[Kc1_padded_row_size, const cudaStream_t & stream) {\n",
            "      | \u001b[01;35m\u001b[K^\u001b[m\u001b[K   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_vec_sparse_dequantized(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:7251:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ksrc1_ddq_i\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 7250 |     const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i\u001b[01;35m\u001b[K, const float * src1_ddf_i,\u001b[m\u001b[K\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            " 7251 | \u001b[01;35m\u001b[K    const \u001b[m\u001b[Kchar * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n",
            "      | \u001b[01;35m\u001b[K^\u001b[m\u001b[K   \u001b[01;35m\u001b[K~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_transpose_select_gemm(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:7452:91:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst float*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kfloat*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 7452 | ose_cont<<< numBlocks, blockSize, 0, stream>>>((float *)src0_ddf_i, transpose, n\u001b[01;35m\u001b[Ke00, ne01, 1, ne00,\u001b[m\u001b[K ne01,NULL);\n",
            "      |                                                                                 \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K At global scope:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:8771:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Kvoid ggml_cuda_axpy(const ggml_tensor*, const ggml_tensor*, ggml_tensor*)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 8771 | void \u001b[01;35m\u001b[Kggml_cuda_axpy\u001b[m\u001b[K(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n",
            "      |      \u001b[01;35m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_transpose_gemm(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:7550:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ksrc0_ddq_as_f32\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 7550 | \u001b[01;35m\u001b[K        ggml_cuda_pool_free(src0_ddq_as_f32, \u001b[m\u001b[Ksrc0_as);\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:7501:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ksrc0_ddq_as_f32\u001b[m\u001b[K’ was declared here\n",
            " 7501 |     flo\u001b[01;36m\u001b[Kat * src0_ddq_a\u001b[m\u001b[Ks_f32;\n",
            "      |        \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[  5%] Built target ggml\n",
            "[  6%] \u001b[32m\u001b[1mLinking CUDA static library libggml_static.a\u001b[0m\n",
            "[  6%] Built target ggml_static\n",
            "[  7%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:632:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Ktensor_offloading_levels get_offloading_level(llm_tensor)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  632 | tensor_offloading_levels \u001b[01;35m\u001b[Kget_offloading_level\u001b[m\u001b[K(llm_tensor tensor) {\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint64_t sum_gpu_index(ggml_tensor*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2722:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kggml_init_params::mem_buffer\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2722 |     ggml_context * ctx_aux = \u001b[01;35m\u001b[Kggml_init({\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~^~\u001b[m\u001b[K\n",
            " 2723 | \u001b[01;35m\u001b[K        /* mem_size */ 1 << 10,\u001b[m\u001b[K\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K        \n",
            " 2724 | \u001b[01;35m\u001b[K    })\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K~~\u001b[m\u001b[K                                 \n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2722:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kggml_init_params::no_alloc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2805:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kprogress\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2805 |         llama_progress_callback cb = [](\u001b[01;35m\u001b[Kfloat progress\u001b[m\u001b[K, void *ctx) {\n",
            "      |                                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2805:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kctx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2805 |         llama_progress_callback cb = [](float progress, \u001b[01;35m\u001b[Kvoid *ctx\u001b[m\u001b[K) {\n",
            "      |                                                         \u001b[01;35m\u001b[K~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ksize_t llama_augmentation_model_loader::slice_ffn_mat_to_gpu(llama_layer&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2909:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kgpu_idx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2909 |         ggml_tensor * \u001b[01;35m\u001b[Kgpu_idx\u001b[m\u001b[K = layer.gpu_idx;\n",
            "      |                       \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llm_load_sparse_model_tensors(llama_model_loader&, llama_model&, const llama_context_params*, int, long int, bool, bool, bool, llama_progress_callback, void*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:3165:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3165 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
            "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:3166:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3166 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
            "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_reserve_model_kv_cache(llama_model*, const llama_context_params*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:3319:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3319 |     if (\u001b[01;35m\u001b[Kmodel->n_gpu_layers < hparams.n_layer + 1\u001b[m\u001b[K) {\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::pair<ggml_tensor*, ggml_tensor*> llm_build_kv_store(ggml_context*, const llama_hparams&, const llama_kv_cache&, ggml_cgraph*, ggml_tensor*, ggml_tensor*, int64_t, int32_t, int32_t, const llm_build_cb&, int64_t)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:4232:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kgraph\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 4232 |          \u001b[01;35m\u001b[Kstruct ggml_cgraph * graph\u001b[m\u001b[K,\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:4677:88:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Knl\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 4677 | onst llm_build_cb no_offload_cb = [](struct ggml_tensor * cur, const char * name, \u001b[01;35m\u001b[Kint nl\u001b[m\u001b[K) {\n",
            "      |                                                                                   \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
            "\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint llama_decode_internal(llama_context&, llama_batch)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:6592:16:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kfull_offload_supported\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 6592 |     const bool \u001b[01;35m\u001b[Kfull_offload_supported\u001b[m\u001b[K =\n",
            "      |                \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kllama_model_params llama_model_default_params()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:9400:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::reset_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 9400 |     \u001b[01;35m\u001b[K}\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:9400:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::disable_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
            "[  8%] Built target llama\n",
            "[  9%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 10%] Built target build_info\n",
            "[ 12%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 17%] Built target common\n",
            "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 19%] Built target test-quantize-fns\n",
            "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 21%] Built target test-quantize-perf\n",
            "[ 23%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 24%] Built target test-sampling\n",
            "[ 25%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-llama\u001b[0m\n",
            "[ 26%] Built target test-tokenizer-0-llama\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-falcon\u001b[0m\n",
            "[ 28%] Built target test-tokenizer-0-falcon\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-llama\u001b[0m\n",
            "[ 30%] Built target test-tokenizer-1-llama\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 32%] Built target test-tokenizer-1-bpe\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 35%] Built target test-grammar-parser\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/content/PowerInfer/tests/test-llama-grammar.cpp:5\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:632:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Ktensor_offloading_levels get_offloading_level(llm_tensor)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  632 | tensor_offloading_levels \u001b[01;35m\u001b[Kget_offloading_level\u001b[m\u001b[K(llm_tensor tensor) {\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/PowerInfer/tests/test-llama-grammar.cpp:5\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint64_t sum_gpu_index(ggml_tensor*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2722:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kggml_init_params::mem_buffer\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2722 |     ggml_context * ctx_aux = \u001b[01;35m\u001b[Kggml_init({\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~^~\u001b[m\u001b[K\n",
            " 2723 | \u001b[01;35m\u001b[K        /* mem_size */ 1 << 10,\u001b[m\u001b[K\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K        \n",
            " 2724 | \u001b[01;35m\u001b[K    })\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K~~\u001b[m\u001b[K                                 \n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2722:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kggml_init_params::no_alloc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2805:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kprogress\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2805 |         llama_progress_callback cb = [](\u001b[01;35m\u001b[Kfloat progress\u001b[m\u001b[K, void *ctx) {\n",
            "      |                                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2805:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kctx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2805 |         llama_progress_callback cb = [](float progress, \u001b[01;35m\u001b[Kvoid *ctx\u001b[m\u001b[K) {\n",
            "      |                                                         \u001b[01;35m\u001b[K~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ksize_t llama_augmentation_model_loader::slice_ffn_mat_to_gpu(llama_layer&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2909:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kgpu_idx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2909 |         ggml_tensor * \u001b[01;35m\u001b[Kgpu_idx\u001b[m\u001b[K = layer.gpu_idx;\n",
            "      |                       \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llm_load_sparse_model_tensors(llama_model_loader&, llama_model&, const llama_context_params*, int, long int, bool, bool, bool, llama_progress_callback, void*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:3165:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3165 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
            "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:3166:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3166 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
            "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_reserve_model_kv_cache(llama_model*, const llama_context_params*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:3319:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3319 |     if (\u001b[01;35m\u001b[Kmodel->n_gpu_layers < hparams.n_layer + 1\u001b[m\u001b[K) {\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::pair<ggml_tensor*, ggml_tensor*> llm_build_kv_store(ggml_context*, const llama_hparams&, const llama_kv_cache&, ggml_cgraph*, ggml_tensor*, ggml_tensor*, int64_t, int32_t, int32_t, const llm_build_cb&, int64_t)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:4232:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kgraph\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 4232 |          \u001b[01;35m\u001b[Kstruct ggml_cgraph * graph\u001b[m\u001b[K,\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:4677:88:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Knl\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 4677 | onst llm_build_cb no_offload_cb = [](struct ggml_tensor * cur, const char * name, \u001b[01;35m\u001b[Kint nl\u001b[m\u001b[K) {\n",
            "      |                                                                                   \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
            "\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint llama_decode_internal(llama_context&, llama_batch)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:6592:16:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kfull_offload_supported\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 6592 |     const bool \u001b[01;35m\u001b[Kfull_offload_supported\u001b[m\u001b[K =\n",
            "      |                \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kllama_model_params llama_model_default_params()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:9400:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::reset_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 9400 |     \u001b[01;35m\u001b[K}\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:9400:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::disable_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 37%] Built target test-llama-grammar\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n",
            "[ 39%] Built target test-grad0\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 41%] Built target test-rope\n",
            "[ 42%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-c\u001b[0m\n",
            "[ 43%] Built target test-c\n",
            "[ 45%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
            "[ 46%] Built target baby-llama\n",
            "[ 47%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched\u001b[0m\n",
            "[ 48%] Built target batched\n",
            "[ 49%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched-bench\u001b[0m\n",
            "[ 50%] Built target batched-bench\n",
            "[ 51%] \u001b[32mBuilding CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/beam-search\u001b[0m\n",
            "[ 52%] Built target beam-search\n",
            "[ 53%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
            "[ 54%] Built target benchmark\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/convert-llama2c-to-ggml\u001b[0m\n",
            "[ 57%] Built target convert-llama2c-to-ggml\n",
            "[ 58%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
            "[ 59%] Built target embedding\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/finetune\u001b[0m\n",
            "[ 61%] Built target finetune\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/infill\u001b[0m\n",
            "[ 63%] Built target infill\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 65%] Built target llama-bench\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/llava/llava.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool load_file_to_bytes(const char*, unsigned char**, long int*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/llava/llava.cpp:130:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  130 |     \u001b[01;35m\u001b[Kfread(buffer, 1, fileSize, file)\u001b[m\u001b[K; // Read the file into the buffer\n",
            "      |     \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 68%] Built target llava\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 69%] Built target llava_static\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llava-cli\u001b[0m\n",
            "[ 71%] Built target llava-cli\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
            "[ 73%] Built target main\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/parallel\u001b[0m\n",
            "[ 75%] Built target parallel\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
            "[ 78%] Built target perplexity\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
            "[ 80%] Built target quantize\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
            "[ 82%] Built target quantize-stats\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
            "[ 84%] Built target save-load-state\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/simple\u001b[0m\n",
            "[ 86%] Built target simple\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/speculative\u001b[0m\n",
            "[ 89%] Built target speculative\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/train-text-from-scratch\u001b[0m\n",
            "[ 91%] Built target train-text-from-scratch\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/server.dir/server.cpp.o\u001b[0m\n",
            "In copy constructor ‘\u001b[01m\u001b[Ktask_result::task_result(const task_result&)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_result; _Args = {const task_result&}; _Tp = task_result]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_result; _Args = {const task_result&}; _Tp = task_result]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_result; _Alloc = std::allocator<task_result>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid llama_server_context::send_error(int, std::string)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1097:32\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:154:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kres.task_result::stop\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  154 | struct \u001b[01;35m\u001b[Ktask_result\u001b[m\u001b[K {\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid llama_server_context::send_error(int, std::string)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1093:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kres\u001b[m\u001b[K’ declared here\n",
            " 1093 |         task_result \u001b[01;36m\u001b[Kres\u001b[m\u001b[K;\n",
            "      |                     \u001b[01;36m\u001b[K^~~\u001b[m\u001b[K\n",
            "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:2355:61\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
            " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
            "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:2410:61\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
            " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
            "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:2514:61\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
            " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
            "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/server\u001b[0m\n",
            "[ 93%] Built target server\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/export-lora\u001b[0m\n",
            "[ 95%] Built target export-lora\n",
            "[ 96%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
            "[ 97%] Built target vdot\n",
            "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
            "[100%] Built target q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ReluLLaMA-7B-PowerInfer-GGUF\n",
        "!huggingface-cli download --resume-download --local-dir ReluLLaMA-7B-PowerInfer-GGUF --local-dir-use-symlinks False PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaEPObBW01wx",
        "outputId": "744e3de3-6123-4703-c0a6-02f8b3d1f058",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Fetching 37 files:   0% 0/37 [00:00<?, ?it/s]Downloading 'README.md' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/README.md.a442128e379d72e41dc23c2de5dc6e267791acc4.incomplete'\n",
            "Downloading 'activation/activation_0.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_0.pt.835820d772d7a56a47bebe134fa8aacf0c9c1c20c29f30f168d95572f885efb6.incomplete'\n",
            "Downloading '.gitignore' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/.gitignore.e3112021e91eb7843c7b47cca43afb7661c58037.incomplete'\n",
            "Downloading '.gitattributes' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/.gitattributes.49297a765b07d7cac725aae792f0eb7782fd4bda.incomplete'\n",
            "Downloading 'activation/activation_11.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_11.pt.b707589d014a27ec71724278c6da0d9b73855e074d9b5f47235d0718f409993e.incomplete'\n",
            "Downloading 'activation/activation_1.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_1.pt.90feac4a9135e5418a179cbe2e06c144ea2198f9df0ef0172303eceae7d4ba1d.incomplete'\n",
            "Downloading 'activation/activation_12.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_12.pt.abeb445e3a38d03f7f30f3d1509d9bd909b37363130787c95251e54bc6e30ee2.incomplete'\n",
            "Downloading 'activation/activation_10.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_10.pt.e90aa71b6eb6c835409289bfda4ac1e7312ecc8e957aab766d84740500d2ac3c.incomplete'\n",
            "\n",
            "README.md: 100% 442/442 [00:00<00:00, 2.12MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/README.md\n",
            "\n",
            ".gitignore: 100% 9.00/9.00 [00:00<00:00, 57.8kB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/.gitignore\n",
            "\n",
            ".gitattributes: 100% 1.58k/1.58k [00:00<00:00, 11.4MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/.gitattributes\n",
            "Fetching 37 files:   3% 1/37 [00:00<00:35,  1.02it/s]\n",
            "activation_0.pt: 100% 88.8k/88.8k [00:00<00:00, 39.5MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_0.pt\n",
            "\n",
            "activation_11.pt: 100% 88.8k/88.8k [00:00<00:00, 154MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_11.pt\n",
            "\n",
            "activation_1.pt: 100% 88.8k/88.8k [00:00<00:00, 26.0MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_1.pt\n",
            "Fetching 37 files:  14% 5/37 [00:01<00:05,  5.74it/s]\n",
            "activation_10.pt: 100% 88.8k/88.8k [00:00<00:00, 27.7MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_10.pt\n",
            "\n",
            "activation_12.pt: 100% 88.8k/88.8k [00:00<00:00, 27.3MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_12.pt\n",
            "Downloading 'activation/activation_14.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_14.pt.596efa0c65c100a72db84164af2ede04e3ea1635cfb8f7f893bb04a1d5f585e3.incomplete'\n",
            "Downloading 'activation/activation_15.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_15.pt.f9bdf525404428d0efb7be8676efca38d2d68722e2190fd455d694f44e6d0edc.incomplete'\n",
            "Downloading 'activation/activation_13.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_13.pt.285ce26c878b06c99cce051dfe4e8035537585f8e26c891a868d6b5ff27dfd52.incomplete'\n",
            "Downloading 'activation/activation_16.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_16.pt.81791115bcecb60e52e6eccf13b37073ee80853ea4f6b39dfe5380e84dc4c546.incomplete'\n",
            "\n",
            "activation_16.pt: 100% 88.8k/88.8k [00:00<00:00, 172MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_16.pt\n",
            "Downloading 'activation/activation_17.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_17.pt.b3220961d235e1f379f313cea3c5e47b3932ef8437fe81f7b633622dd775b16a.incomplete'\n",
            "\n",
            "activation_17.pt: 100% 88.8k/88.8k [00:00<00:00, 49.0MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_17.pt\n",
            "Downloading 'activation/activation_18.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_18.pt.4659fbaa04c295dd0fe87f49e6a8f6de99dbd4452927a9486f42b4e1acba9b70.incomplete'\n",
            "\n",
            "activation_18.pt:   0% 0.00/88.8k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "activation_15.pt:   0% 0.00/88.8k [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'activation/activation_19.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_19.pt.33bbdabcc7dc77955c8ba1bef60dc6625c8270017c061dc277dfb5cd29bf9603.incomplete'\n",
            "activation_15.pt: 100% 88.8k/88.8k [00:00<00:00, 16.6MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_15.pt\n",
            "\n",
            "\n",
            "activation_14.pt:   0% 0.00/88.8k [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'activation/activation_2.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_2.pt.e76b13ed0b7d869bbd25ce9d21cbc20970aa11f7998a722b8912cdd59e962504.incomplete'\n",
            "activation_14.pt: 100% 88.8k/88.8k [00:00<00:00, 29.4MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_14.pt\n",
            "\n",
            "\n",
            "activation_19.pt: 100% 88.8k/88.8k [00:00<00:00, 205MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_19.pt\n",
            "\n",
            "\n",
            "activation_18.pt: 100% 88.8k/88.8k [00:00<00:00, 3.79MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_18.pt\n",
            "activation_2.pt: 100% 88.8k/88.8k [00:00<00:00, 40.4MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_2.pt\n",
            "\n",
            "activation_13.pt: 100% 88.8k/88.8k [00:00<00:00, 28.3MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_13.pt\n",
            "Fetching 37 files:  24% 9/37 [00:01<00:03,  8.39it/s]Downloading 'activation/activation_20.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_20.pt.8df5cc35fbccd12ae2a7d45af6a519cca8d68192b021f8a59b51b136e45d7825.incomplete'\n",
            "\n",
            "activation_20.pt: 100% 88.8k/88.8k [00:00<00:00, 156MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_20.pt\n",
            "Fetching 37 files:  46% 17/37 [00:01<00:01, 16.71it/s]Downloading 'activation/activation_21.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_21.pt.f4848d158bc4b24805db39c33b18d8b1bbde1110fecfc6916f8ced5a633388c5.incomplete'\n",
            "\n",
            "activation_21.pt: 100% 88.8k/88.8k [00:00<00:00, 187MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_21.pt\n",
            "Downloading 'activation/activation_22.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_22.pt.b9de4dbbb1821da6a364bfd0863656ca271e4af1d68e0cec277baa5988a7f008.incomplete'\n",
            "Downloading 'activation/activation_24.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_24.pt.c40efbc310e82fb6a8951c303f9f95b576f787f877cb9f333d5db4c60b40a333.incomplete'\n",
            "Downloading 'activation/activation_23.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_23.pt.88db40cfbdb55d35f90c2f3a2909ae569184f59acb1a5333952a75d1b3b91237.incomplete'\n",
            "Downloading 'activation/activation_26.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_26.pt.c2dec60229b774fd350e7a724cf279927327836c4cecf71003bcb9d3c6cc7ff6.incomplete'\n",
            "Downloading 'activation/activation_27.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_27.pt.762be6b4779874100ea7d3fe31546a768c485160d7a2d7a2d8d84e606eba8d69.incomplete'\n",
            "Downloading 'activation/activation_25.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_25.pt.af8d82cb185da97e8240fd589e0b096cab3a06b01497889c11bc5e769b4d1c18.incomplete'\n",
            "\n",
            "activation_24.pt: 100% 88.8k/88.8k [00:00<00:00, 157MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_24.pt\n",
            "\n",
            "activation_22.pt: 100% 88.8k/88.8k [00:00<00:00, 228MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_22.pt\n",
            "\n",
            "activation_26.pt:   0% 0.00/88.8k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "activation_25.pt:   0% 0.00/88.8k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "activation_26.pt: 100% 88.8k/88.8k [00:00<00:00, 18.1MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_26.pt\n",
            "activation_25.pt: 100% 88.8k/88.8k [00:00<00:00, 47.5MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_25.pt\n",
            "activation_27.pt: 100% 88.8k/88.8k [00:00<00:00, 21.3MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_27.pt\n",
            "\n",
            "activation_23.pt: 100% 88.8k/88.8k [00:00<00:00, 217MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_23.pt\n",
            "Fetching 37 files:  54% 20/37 [00:01<00:00, 18.20it/s]Downloading 'activation/activation_28.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_28.pt.c957ba5f46d5603ebc5cbabcad318e00a9d106e545f04b1b53e60f1923328cfc.incomplete'\n",
            "Downloading 'activation/activation_29.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_29.pt.80d6f33cbacae22d9827c0ebe1979bb65d3c95d636987c21600048b18c88f5d6.incomplete'\n",
            "\n",
            "activation_28.pt: 100% 88.8k/88.8k [00:00<00:00, 86.4MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_28.pt\n",
            "Fetching 37 files:  68% 25/37 [00:01<00:00, 21.76it/s]\n",
            "activation_29.pt: 100% 88.8k/88.8k [00:00<00:00, 192MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_29.pt\n",
            "Downloading 'activation/activation_30.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_30.pt.8840b9ac20ded86e75ec8cd4e241314167a3f141831032604340863bc5773b32.incomplete'\n",
            "Downloading 'activation/activation_4.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_4.pt.7561173df0c6df5f169d81864a4a508825a45c1b4bca96c8aedcd2bc86657558.incomplete'\n",
            "Downloading 'activation/activation_3.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_3.pt.2de431184f5d127f4049d6453130de71ecdfaec0cafb77fbea5ae3ce471de45a.incomplete'\n",
            "Downloading 'activation/activation_5.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_5.pt.bfe19b9da73e27e92568828922479ba1aa18cca0674421d33e3c6ea6c01fee60.incomplete'\n",
            "Downloading 'activation/activation_6.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_6.pt.958df5980de566590cb8c579c980f6e37eeba6bad68afceae936489c882c6771.incomplete'\n",
            "Downloading 'activation/activation_31.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_31.pt.28bf73e17d8eca05d3d01af90804ef4a76f576a9a1f99ad9ada3aac49af1d36d.incomplete'\n",
            "\n",
            "activation_30.pt: 100% 88.8k/88.8k [00:00<00:00, 190MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_30.pt\n",
            "\n",
            "activation_5.pt:   0% 0.00/88.8k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "activation_3.pt:   0% 0.00/88.8k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "activation_31.pt: 100% 88.8k/88.8k [00:00<00:00, 120MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_31.pt\n",
            "\n",
            "\n",
            "\n",
            "activation_6.pt: 100% 88.8k/88.8k [00:00<00:00, 217MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_6.pt\n",
            "\n",
            "\n",
            "\n",
            "activation_4.pt: 100% 88.8k/88.8k [00:00<00:00, 208MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_4.pt\n",
            "activation_5.pt: 100% 88.8k/88.8k [00:00<00:00, 13.2MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_5.pt\n",
            "activation_3.pt: 100% 88.8k/88.8k [00:00<00:00, 13.8MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_3.pt\n",
            "Fetching 37 files:  76% 28/37 [00:02<00:00, 21.26it/s]Downloading 'activation/activation_7.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_7.pt.36191b81c241f7037b81762105c15114b785e8d1a176370901c272db6d27371f.incomplete'\n",
            "\n",
            "activation_7.pt: 100% 88.8k/88.8k [00:00<00:00, 210MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_7.pt\n",
            "Downloading 'activation/activation_8.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_8.pt.ca6787c57518e99161f660d08bda68f36ffd76a8f6424f98a544c8d8ab2b10d6.incomplete'\n",
            "\n",
            "activation_8.pt: 100% 88.8k/88.8k [00:00<00:00, 202MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_8.pt\n",
            "Fetching 37 files:  92% 34/37 [00:02<00:00, 28.11it/s]Downloading 'llama-7b-relu.powerinfer.gguf' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/llama-7b-relu.powerinfer.gguf.ed59b0524bc7febf5a6288d8c52fb249a7bed2b23089286af2e97a5d18169086.incomplete'\n",
            "Downloading 'activation/activation_9.pt' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/activation/activation_9.pt.95ac2654c2d36c5fd37d36c5c77d77484a18632170415394fbf3ae3f5f3583de.incomplete'\n",
            "Downloading 'config.json' to 'ReluLLaMA-7B-PowerInfer-GGUF/.huggingface/download/config.json.352d34f6e153f27fbd7fd9c7f9158a1619c66da5.incomplete'\n",
            "\n",
            "activation_9.pt: 100% 88.8k/88.8k [00:00<00:00, 21.6MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/activation/activation_9.pt\n",
            "\n",
            "llama-7b-relu.powerinfer.gguf:   0% 0.00/15.1G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "config.json: 100% 30.0/30.0 [00:00<00:00, 147kB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/config.json\n",
            "\n",
            "llama-7b-relu.powerinfer.gguf:   0% 10.5M/15.1G [00:00<15:36, 16.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   0% 21.0M/15.1G [00:01<12:26, 20.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   0% 31.5M/15.1G [00:01<11:25, 22.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   0% 41.9M/15.1G [00:01<10:53, 23.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   0% 52.4M/15.1G [00:02<10:38, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   0% 62.9M/15.1G [00:02<10:31, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   0% 73.4M/15.1G [00:03<10:24, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 83.9M/15.1G [00:03<10:23, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 94.4M/15.1G [00:04<10:17, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 105M/15.1G [00:04<10:14, 24.5MB/s] \u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 115M/15.1G [00:04<10:13, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 126M/15.1G [00:05<10:12, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 136M/15.1G [00:05<10:11, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 147M/15.1G [00:06<10:10, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 157M/15.1G [00:06<10:10, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 168M/15.1G [00:07<10:10, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 178M/15.1G [00:07<10:10, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 189M/15.1G [00:07<10:09, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 199M/15.1G [00:08<10:08, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 210M/15.1G [00:08<10:07, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   1% 220M/15.1G [00:09<10:06, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 231M/15.1G [00:09<10:07, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 241M/15.1G [00:10<10:07, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 252M/15.1G [00:10<10:06, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 262M/15.1G [00:10<10:06, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 273M/15.1G [00:11<10:05, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 283M/15.1G [00:11<10:06, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 294M/15.1G [00:12<10:06, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 304M/15.1G [00:12<10:05, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 315M/15.1G [00:13<10:04, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 325M/15.1G [00:13<10:03, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 336M/15.1G [00:13<10:02, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 346M/15.1G [00:14<10:02, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 357M/15.1G [00:14<10:02, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 367M/15.1G [00:15<10:01, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   2% 377M/15.1G [00:15<10:00, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 388M/15.1G [00:16<09:59, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 398M/15.1G [00:16<09:59, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 409M/15.1G [00:16<09:58, 24.6MB/s]\u001b[A\n",
            "Fetching 37 files:  92% 34/37 [00:19<00:00, 28.11it/s]\n",
            "llama-7b-relu.powerinfer.gguf:   3% 430M/15.1G [00:17<10:54, 22.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 440M/15.1G [00:18<10:41, 22.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 451M/15.1G [00:18<10:30, 23.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 461M/15.1G [00:19<10:24, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 472M/15.1G [00:19<10:17, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 482M/15.1G [00:20<10:10, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 493M/15.1G [00:20<10:06, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 503M/15.1G [00:20<10:03, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 514M/15.1G [00:21<10:00, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   3% 524M/15.1G [00:21<09:59, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 535M/15.1G [00:22<09:57, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 545M/15.1G [00:22<09:56, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 556M/15.1G [00:22<09:55, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 566M/15.1G [00:23<09:54, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 577M/15.1G [00:23<09:53, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 587M/15.1G [00:24<09:53, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 598M/15.1G [00:24<09:53, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 608M/15.1G [00:25<09:53, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 619M/15.1G [00:25<09:51, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 629M/15.1G [00:25<09:50, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 640M/15.1G [00:26<09:51, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 650M/15.1G [00:26<09:51, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 661M/15.1G [00:27<09:51, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 671M/15.1G [00:27<09:50, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   4% 682M/15.1G [00:28<09:50, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 692M/15.1G [00:28<09:48, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 703M/15.1G [00:28<09:47, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 713M/15.1G [00:29<09:47, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 724M/15.1G [00:29<09:47, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 734M/15.1G [00:30<09:46, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 744M/15.1G [00:30<09:46, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 755M/15.1G [00:31<09:45, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 765M/15.1G [00:31<09:44, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 776M/15.1G [00:31<09:43, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 786M/15.1G [00:32<09:49, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 797M/15.1G [00:32<09:43, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 807M/15.1G [00:33<09:43, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 818M/15.1G [00:33<09:42, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   5% 828M/15.1G [00:34<09:41, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 839M/15.1G [00:34<09:40, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 849M/15.1G [00:34<09:41, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 860M/15.1G [00:35<10:20, 23.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 870M/15.1G [00:35<10:15, 23.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 881M/15.1G [00:36<10:12, 23.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 891M/15.1G [00:36<10:01, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 902M/15.1G [00:37<09:55, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 912M/15.1G [00:37<09:51, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 923M/15.1G [00:38<09:47, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 933M/15.1G [00:38<09:46, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 944M/15.1G [00:38<09:46, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 954M/15.1G [00:39<09:45, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 965M/15.1G [00:39<09:43, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   6% 975M/15.1G [00:40<09:40, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 986M/15.1G [00:40<09:39, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 996M/15.1G [00:41<09:37, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.01G/15.1G [00:41<09:36, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.02G/15.1G [00:41<09:35, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.03G/15.1G [00:42<09:35, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.04G/15.1G [00:42<09:34, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.05G/15.1G [00:43<09:35, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.06G/15.1G [00:43<09:35, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.07G/15.1G [00:44<09:34, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.08G/15.1G [00:44<09:33, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.09G/15.1G [00:44<09:32, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.10G/15.1G [00:45<09:31, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.11G/15.1G [00:45<09:30, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.12G/15.1G [00:46<09:31, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   7% 1.13G/15.1G [00:46<09:32, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.14G/15.1G [00:47<09:47, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.15G/15.1G [00:47<10:03, 23.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.16G/15.1G [00:48<09:52, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.17G/15.1G [00:48<09:45, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.18G/15.1G [00:48<09:39, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.20G/15.1G [00:49<09:36, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.21G/15.1G [00:49<09:32, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.22G/15.1G [00:50<09:31, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.23G/15.1G [00:50<09:29, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.24G/15.1G [00:50<09:27, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.25G/15.1G [00:51<09:31, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.26G/15.1G [00:51<09:29, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.27G/15.1G [00:52<09:28, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   8% 1.28G/15.1G [00:52<09:27, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.29G/15.1G [00:53<09:25, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.30G/15.1G [00:53<09:25, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.31G/15.1G [00:53<09:23, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.32G/15.1G [00:54<09:23, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.33G/15.1G [00:54<09:26, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.34G/15.1G [00:55<09:26, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.35G/15.1G [00:55<09:27, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.36G/15.1G [00:56<09:24, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.37G/15.1G [00:56<09:24, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.38G/15.1G [00:57<09:25, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.39G/15.1G [00:57<09:24, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.41G/15.1G [00:57<09:23, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.42G/15.1G [00:58<09:23, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.43G/15.1G [00:58<09:23, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:   9% 1.44G/15.1G [00:59<09:21, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.45G/15.1G [00:59<09:19, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.46G/15.1G [01:00<09:18, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.47G/15.1G [01:00<09:17, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.48G/15.1G [01:00<09:16, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.49G/15.1G [01:01<09:15, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.50G/15.1G [01:01<09:15, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.51G/15.1G [01:02<09:29, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.52G/15.1G [01:02<09:24, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.53G/15.1G [01:03<09:38, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.54G/15.1G [01:03<09:33, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.55G/15.1G [01:03<09:28, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.56G/15.1G [01:04<09:25, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.57G/15.1G [01:04<09:21, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  10% 1.58G/15.1G [01:05<09:22, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.59G/15.1G [01:05<09:21, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.60G/15.1G [01:06<09:16, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.61G/15.1G [01:06<09:16, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.63G/15.1G [01:06<09:16, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.64G/15.1G [01:07<09:13, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.65G/15.1G [01:07<09:13, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.66G/15.1G [01:08<09:11, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.67G/15.1G [01:08<09:09, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.68G/15.1G [01:09<09:10, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.69G/15.1G [01:09<09:09, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.70G/15.1G [01:09<09:08, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.71G/15.1G [01:10<09:26, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.72G/15.1G [01:10<09:18, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.73G/15.1G [01:11<09:14, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  11% 1.74G/15.1G [01:11<09:11, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.75G/15.1G [01:12<09:09, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.76G/15.1G [01:12<09:07, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.77G/15.1G [01:12<09:05, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.78G/15.1G [01:13<09:04, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.79G/15.1G [01:13<09:04, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.80G/15.1G [01:14<09:02, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.81G/15.1G [01:14<09:02, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.82G/15.1G [01:15<09:01, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.84G/15.1G [01:15<09:00, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.85G/15.1G [01:15<09:10, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.86G/15.1G [01:16<09:10, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.87G/15.1G [01:16<09:06, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.88G/15.1G [01:17<09:04, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  12% 1.89G/15.1G [01:17<09:02, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.90G/15.1G [01:18<09:12, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.91G/15.1G [01:18<09:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.92G/15.1G [01:19<09:07, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.93G/15.1G [01:19<09:06, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.94G/15.1G [01:19<09:04, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.95G/15.1G [01:20<09:01, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.96G/15.1G [01:20<09:02, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.97G/15.1G [01:21<09:07, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.98G/15.1G [01:21<09:13, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 1.99G/15.1G [01:22<09:08, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 2.00G/15.1G [01:22<09:05, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 2.01G/15.1G [01:22<09:01, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 2.02G/15.1G [01:23<08:59, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 2.03G/15.1G [01:23<08:59, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  13% 2.04G/15.1G [01:24<09:00, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.06G/15.1G [01:24<08:58, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.07G/15.1G [01:25<08:58, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.08G/15.1G [01:25<08:58, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.09G/15.1G [01:25<08:56, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.10G/15.1G [01:26<08:55, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.11G/15.1G [01:26<08:52, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.12G/15.1G [01:27<08:51, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.13G/15.1G [01:27<08:51, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.14G/15.1G [01:28<08:49, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.15G/15.1G [01:28<08:49, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.16G/15.1G [01:28<08:56, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.17G/15.1G [01:29<08:53, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.18G/15.1G [01:29<08:52, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  14% 2.19G/15.1G [01:30<08:51, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.20G/15.1G [01:30<08:48, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.21G/15.1G [01:31<08:48, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.22G/15.1G [01:31<08:47, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.23G/15.1G [01:31<08:46, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.24G/15.1G [01:32<08:47, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.25G/15.1G [01:32<08:53, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.26G/15.1G [01:33<08:49, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.28G/15.1G [01:33<08:47, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.29G/15.1G [01:34<08:48, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.30G/15.1G [01:34<08:48, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.31G/15.1G [01:34<08:46, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.32G/15.1G [01:35<08:45, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.33G/15.1G [01:35<08:47, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  15% 2.34G/15.1G [01:36<08:47, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.35G/15.1G [01:36<08:49, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.36G/15.1G [01:37<08:46, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.37G/15.1G [01:37<08:53, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.38G/15.1G [01:37<08:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.39G/15.1G [01:38<08:47, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.40G/15.1G [01:38<08:44, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.41G/15.1G [01:39<08:48, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.42G/15.1G [01:39<08:47, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.43G/15.1G [01:40<08:44, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.44G/15.1G [01:40<08:45, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.45G/15.1G [01:41<08:43, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.46G/15.1G [01:41<08:40, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.47G/15.1G [01:41<08:38, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.49G/15.1G [01:42<08:37, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  16% 2.50G/15.1G [01:42<08:36, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.51G/15.1G [01:43<08:36, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.52G/15.1G [01:43<08:39, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.53G/15.1G [01:44<08:36, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.54G/15.1G [01:44<08:37, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.55G/15.1G [01:44<08:37, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.56G/15.1G [01:45<08:37, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.57G/15.1G [01:45<08:36, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.58G/15.1G [01:46<08:34, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.59G/15.1G [01:46<08:32, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.60G/15.1G [01:47<08:32, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.61G/15.1G [01:47<08:35, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.62G/15.1G [01:47<08:35, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.63G/15.1G [01:48<08:32, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  17% 2.64G/15.1G [01:48<08:33, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.65G/15.1G [01:49<08:31, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.66G/15.1G [01:49<08:31, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.67G/15.1G [01:50<08:29, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.68G/15.1G [01:50<08:28, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.69G/15.1G [01:50<08:30, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.71G/15.1G [01:51<08:29, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.72G/15.1G [01:51<08:33, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.73G/15.1G [01:52<08:32, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.74G/15.1G [01:52<08:29, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.75G/15.1G [01:53<08:35, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.76G/15.1G [01:53<08:36, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.77G/15.1G [01:53<08:33, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.78G/15.1G [01:54<08:35, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.79G/15.1G [01:54<08:31, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  18% 2.80G/15.1G [01:55<08:28, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.81G/15.1G [01:55<08:25, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.82G/15.1G [01:56<08:24, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.83G/15.1G [01:56<08:24, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.84G/15.1G [01:56<08:25, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.85G/15.1G [01:57<08:23, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.86G/15.1G [01:57<08:22, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.87G/15.1G [01:58<08:23, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.88G/15.1G [01:58<08:22, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.89G/15.1G [01:59<08:22, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.90G/15.1G [01:59<08:21, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.92G/15.1G [01:59<08:19, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.93G/15.1G [02:00<08:21, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.94G/15.1G [02:00<08:21, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  19% 2.95G/15.1G [02:01<08:20, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 2.96G/15.1G [02:01<08:18, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 2.97G/15.1G [02:02<08:22, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 2.98G/15.1G [02:02<08:22, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 2.99G/15.1G [02:02<08:21, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.00G/15.1G [02:03<08:19, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.01G/15.1G [02:03<08:17, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.02G/15.1G [02:04<08:16, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.03G/15.1G [02:04<08:16, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.04G/15.1G [02:05<08:17, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.05G/15.1G [02:05<08:18, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.06G/15.1G [02:05<08:15, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.07G/15.1G [02:06<08:15, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.08G/15.1G [02:06<08:13, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.09G/15.1G [02:07<08:12, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  20% 3.10G/15.1G [02:07<08:21, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.11G/15.1G [02:08<08:19, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.12G/15.1G [02:08<08:22, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.14G/15.1G [02:09<08:17, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.15G/15.1G [02:09<08:17, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.16G/15.1G [02:09<08:14, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.17G/15.1G [02:10<08:13, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.18G/15.1G [02:10<08:14, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.19G/15.1G [02:11<08:08, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.20G/15.1G [02:11<08:07, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.21G/15.1G [02:12<08:06, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.22G/15.1G [02:12<08:07, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.23G/15.1G [02:12<08:10, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.24G/15.1G [02:13<08:09, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  21% 3.25G/15.1G [02:13<08:07, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.26G/15.1G [02:14<08:06, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.27G/15.1G [02:14<08:05, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.28G/15.1G [02:15<08:04, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.29G/15.1G [02:15<08:05, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.30G/15.1G [02:15<08:05, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.31G/15.1G [02:16<08:04, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.32G/15.1G [02:16<08:03, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.33G/15.1G [02:17<08:01, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.34G/15.1G [02:17<08:06, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.36G/15.1G [02:18<08:05, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.37G/15.1G [02:18<08:07, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.38G/15.1G [02:18<08:05, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.39G/15.1G [02:19<08:03, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.40G/15.1G [02:19<08:03, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  22% 3.41G/15.1G [02:20<08:04, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.42G/15.1G [02:20<08:03, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.43G/15.1G [02:21<08:00, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.44G/15.1G [02:21<08:00, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.45G/15.1G [02:21<07:59, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.46G/15.1G [02:22<07:57, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.47G/15.1G [02:22<08:03, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.48G/15.1G [02:23<08:02, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.49G/15.1G [02:23<08:02, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.50G/15.1G [02:24<07:59, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.51G/15.1G [02:24<07:59, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.52G/15.1G [02:24<07:58, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.53G/15.1G [02:25<07:58, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.54G/15.1G [02:25<07:58, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  23% 3.55G/15.1G [02:26<07:57, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.57G/15.1G [02:26<07:55, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.58G/15.1G [02:27<07:55, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.59G/15.1G [02:27<07:54, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.60G/15.1G [02:27<07:52, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.61G/15.1G [02:28<07:51, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.62G/15.1G [02:28<07:52, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.63G/15.1G [02:29<07:52, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.64G/15.1G [02:29<07:53, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.65G/15.1G [02:30<07:53, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.66G/15.1G [02:30<07:51, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.67G/15.1G [02:30<07:52, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.68G/15.1G [02:31<07:51, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.69G/15.1G [02:31<07:51, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  24% 3.70G/15.1G [02:32<07:51, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.71G/15.1G [02:32<07:52, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.72G/15.1G [02:33<07:49, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.73G/15.1G [02:33<07:52, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.74G/15.1G [02:34<07:51, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.75G/15.1G [02:34<07:50, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.76G/15.1G [02:34<07:50, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.77G/15.1G [02:35<07:48, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.79G/15.1G [02:35<07:46, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.80G/15.1G [02:36<07:44, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.81G/15.1G [02:36<07:42, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.82G/15.1G [02:37<07:41, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.83G/15.1G [02:37<07:41, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.84G/15.1G [02:37<07:45, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.85G/15.1G [02:38<07:43, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  25% 3.86G/15.1G [02:38<07:43, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.87G/15.1G [02:39<07:43, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.88G/15.1G [02:39<07:42, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.89G/15.1G [02:40<07:43, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.90G/15.1G [02:40<07:42, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.91G/15.1G [02:40<07:43, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.92G/15.1G [02:41<07:42, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.93G/15.1G [02:41<07:41, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.94G/15.1G [02:42<07:41, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.95G/15.1G [02:42<07:40, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.96G/15.1G [02:43<07:38, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.97G/15.1G [02:43<07:38, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 3.98G/15.1G [02:43<07:39, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 4.00G/15.1G [02:44<07:37, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  26% 4.01G/15.1G [02:44<07:36, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.02G/15.1G [02:45<07:37, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.03G/15.1G [02:45<07:36, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.04G/15.1G [02:46<07:35, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.05G/15.1G [02:46<07:37, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.06G/15.1G [02:46<07:37, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.07G/15.1G [02:47<07:36, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.08G/15.1G [02:47<07:34, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.09G/15.1G [02:48<07:32, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.10G/15.1G [02:48<07:34, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.11G/15.1G [02:49<07:32, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.12G/15.1G [02:49<07:35, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.13G/15.1G [02:49<07:34, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.14G/15.1G [02:50<07:33, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.15G/15.1G [02:50<07:33, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  27% 4.16G/15.1G [02:51<07:32, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.17G/15.1G [02:51<07:30, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.18G/15.1G [02:52<07:28, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.19G/15.1G [02:52<07:27, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.20G/15.1G [02:52<07:31, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.22G/15.1G [02:53<07:29, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.23G/15.1G [02:53<07:29, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.24G/15.1G [02:54<07:28, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.25G/15.1G [02:54<07:28, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.26G/15.1G [02:55<07:26, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.27G/15.1G [02:55<07:25, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.28G/15.1G [02:55<07:23, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.29G/15.1G [02:56<07:25, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.30G/15.1G [02:56<07:25, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  28% 4.31G/15.1G [02:57<07:25, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.32G/15.1G [02:57<07:25, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.33G/15.1G [02:58<07:23, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.34G/15.1G [02:58<07:23, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.35G/15.1G [02:58<07:23, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.36G/15.1G [02:59<07:24, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.37G/15.1G [02:59<07:23, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.38G/15.1G [03:00<07:24, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.39G/15.1G [03:00<07:22, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.40G/15.1G [03:01<07:20, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.41G/15.1G [03:01<07:22, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.42G/15.1G [03:02<07:20, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.44G/15.1G [03:02<07:21, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.45G/15.1G [03:02<07:19, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.46G/15.1G [03:03<07:17, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  29% 4.47G/15.1G [03:03<07:17, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.48G/15.1G [03:04<07:20, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.49G/15.1G [03:04<07:19, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.50G/15.1G [03:05<07:16, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.51G/15.1G [03:05<07:16, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.52G/15.1G [03:05<07:16, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.53G/15.1G [03:06<07:18, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.54G/15.1G [03:06<07:17, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.55G/15.1G [03:07<07:16, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.56G/15.1G [03:07<07:14, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.57G/15.1G [03:08<07:14, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.58G/15.1G [03:08<07:13, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.59G/15.1G [03:08<07:11, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.60G/15.1G [03:09<07:09, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  30% 4.61G/15.1G [03:09<07:14, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.62G/15.1G [03:10<07:14, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.63G/15.1G [03:10<07:13, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.65G/15.1G [03:11<07:11, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.66G/15.1G [03:11<07:10, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.67G/15.1G [03:11<07:09, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.68G/15.1G [03:12<07:10, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.69G/15.1G [03:12<07:09, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.70G/15.1G [03:13<07:08, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.71G/15.1G [03:13<07:10, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.72G/15.1G [03:14<07:08, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.73G/15.1G [03:14<07:07, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.74G/15.1G [03:14<07:08, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.75G/15.1G [03:15<07:06, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.76G/15.1G [03:15<07:07, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  31% 4.77G/15.1G [03:16<07:05, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.78G/15.1G [03:16<07:04, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.79G/15.1G [03:17<07:04, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.80G/15.1G [03:17<07:08, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.81G/15.1G [03:17<07:05, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.82G/15.1G [03:18<07:04, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.83G/15.1G [03:18<07:02, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.84G/15.1G [03:19<07:04, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.85G/15.1G [03:19<07:02, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.87G/15.1G [03:20<07:00, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.88G/15.1G [03:20<07:00, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.89G/15.1G [03:20<07:01, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.90G/15.1G [03:21<07:01, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.91G/15.1G [03:21<06:59, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  32% 4.92G/15.1G [03:22<07:00, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 4.93G/15.1G [03:22<07:01, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 4.94G/15.1G [03:23<06:59, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 4.95G/15.1G [03:23<06:57, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 4.96G/15.1G [03:24<07:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 4.97G/15.1G [03:24<07:00, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 4.98G/15.1G [03:24<06:57, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 4.99G/15.1G [03:25<06:59, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 5.00G/15.1G [03:25<06:57, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 5.01G/15.1G [03:26<06:56, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 5.02G/15.1G [03:26<06:55, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 5.03G/15.1G [03:27<06:55, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 5.04G/15.1G [03:27<06:54, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 5.05G/15.1G [03:27<06:53, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  33% 5.06G/15.1G [03:28<06:51, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.08G/15.1G [03:28<06:53, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.09G/15.1G [03:29<06:52, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.10G/15.1G [03:29<06:51, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.11G/15.1G [03:30<06:53, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.12G/15.1G [03:30<06:54, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.13G/15.1G [03:30<06:55, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.14G/15.1G [03:31<06:52, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.15G/15.1G [03:31<06:50, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.16G/15.1G [03:32<06:49, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.17G/15.1G [03:32<06:49, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.18G/15.1G [03:33<06:48, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.19G/15.1G [03:33<06:47, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.20G/15.1G [03:33<06:46, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.21G/15.1G [03:34<06:56, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  34% 5.22G/15.1G [03:34<06:52, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.23G/15.1G [03:35<06:49, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.24G/15.1G [03:35<06:46, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.25G/15.1G [03:36<06:44, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.26G/15.1G [03:36<06:46, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.27G/15.1G [03:36<06:46, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.28G/15.1G [03:37<06:45, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.30G/15.1G [03:37<06:43, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.31G/15.1G [03:38<06:43, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.32G/15.1G [03:38<06:44, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.33G/15.1G [03:39<06:42, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.34G/15.1G [03:39<06:42, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.35G/15.1G [03:39<06:43, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.36G/15.1G [03:40<06:42, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  35% 5.37G/15.1G [03:40<06:41, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.38G/15.1G [03:41<06:40, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.39G/15.1G [03:41<06:40, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.40G/15.1G [03:42<06:40, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.41G/15.1G [03:42<06:38, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.42G/15.1G [03:42<06:39, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.43G/15.1G [03:43<06:39, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.44G/15.1G [03:43<06:37, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.45G/15.1G [03:44<06:37, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.46G/15.1G [03:44<06:35, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.47G/15.1G [03:45<06:34, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.48G/15.1G [03:45<06:39, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.49G/15.1G [03:45<06:36, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.51G/15.1G [03:46<06:34, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.52G/15.1G [03:46<06:34, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  36% 5.53G/15.1G [03:47<06:34, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.54G/15.1G [03:47<06:35, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.55G/15.1G [03:48<06:35, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.56G/15.1G [03:48<06:33, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.57G/15.1G [03:48<06:32, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.58G/15.1G [03:49<06:30, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.59G/15.1G [03:49<06:34, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.60G/15.1G [03:50<06:36, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.61G/15.1G [03:50<06:35, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.62G/15.1G [03:51<06:34, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.63G/15.1G [03:51<06:33, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.64G/15.1G [03:52<06:33, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.65G/15.1G [03:52<06:31, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.66G/15.1G [03:52<06:29, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  37% 5.67G/15.1G [03:53<06:28, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.68G/15.1G [03:53<06:26, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.69G/15.1G [03:54<06:30, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.70G/15.1G [03:54<06:28, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.71G/15.1G [03:55<06:26, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.73G/15.1G [03:55<06:25, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.74G/15.1G [03:55<06:24, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.75G/15.1G [03:56<06:24, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.76G/15.1G [03:56<06:24, 24.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.77G/15.1G [03:57<06:27, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.78G/15.1G [03:57<06:31, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.79G/15.1G [03:58<06:30, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.80G/15.1G [03:58<06:28, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.81G/15.1G [03:58<06:28, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.82G/15.1G [03:59<06:28, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  38% 5.83G/15.1G [03:59<06:29, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.84G/15.1G [04:00<06:28, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.85G/15.1G [04:00<06:26, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.86G/15.1G [04:01<06:26, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.87G/15.1G [04:01<06:25, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.88G/15.1G [04:01<06:25, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.89G/15.1G [04:02<06:23, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.90G/15.1G [04:02<06:22, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.91G/15.1G [04:03<06:21, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.92G/15.1G [04:03<06:21, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.93G/15.1G [04:04<06:22, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.95G/15.1G [04:04<06:23, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.96G/15.1G [04:05<06:21, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.97G/15.1G [04:05<06:20, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  39% 5.98G/15.1G [04:05<06:20, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 5.99G/15.1G [04:06<06:19, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.00G/15.1G [04:06<06:21, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.01G/15.1G [04:07<06:22, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.02G/15.1G [04:07<06:22, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.03G/15.1G [04:08<06:22, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.04G/15.1G [04:08<06:21, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.05G/15.1G [04:08<06:19, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.06G/15.1G [04:09<06:21, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.07G/15.1G [04:09<06:18, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.08G/15.1G [04:10<06:16, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.09G/15.1G [04:10<06:16, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.10G/15.1G [04:11<06:15, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.11G/15.1G [04:11<06:14, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.12G/15.1G [04:12<06:14, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  40% 6.13G/15.1G [04:12<06:14, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.14G/15.1G [04:12<06:15, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.16G/15.1G [04:13<06:16, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.17G/15.1G [04:13<06:18, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.18G/15.1G [04:14<06:15, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.19G/15.1G [04:14<06:14, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.20G/15.1G [04:15<06:12, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.21G/15.1G [04:15<06:11, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.22G/15.1G [04:15<06:12, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.23G/15.1G [04:16<06:14, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.24G/15.1G [04:16<06:11, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.25G/15.1G [04:17<06:10, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.26G/15.1G [04:17<06:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.27G/15.1G [04:18<06:07, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  41% 6.28G/15.1G [04:18<06:08, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.29G/15.1G [04:19<06:07, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.30G/15.1G [04:19<06:05, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.31G/15.1G [04:19<06:05, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.32G/15.1G [04:20<06:04, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.33G/15.1G [04:20<06:05, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.34G/15.1G [04:21<06:03, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.35G/15.1G [04:21<06:03, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.36G/15.1G [04:22<06:05, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.38G/15.1G [04:22<06:02, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.39G/15.1G [04:22<06:04, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.40G/15.1G [04:23<06:03, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.41G/15.1G [04:23<06:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.42G/15.1G [04:24<06:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  42% 6.43G/15.1G [04:24<06:03, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.44G/15.1G [04:25<06:02, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.45G/15.1G [04:25<06:03, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.46G/15.1G [04:25<06:04, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.47G/15.1G [04:26<06:04, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.48G/15.1G [04:26<06:01, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.49G/15.1G [04:27<06:01, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.50G/15.1G [04:27<06:00, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.51G/15.1G [04:28<06:00, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.52G/15.1G [04:28<05:58, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.53G/15.1G [04:29<06:01, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.54G/15.1G [04:29<05:59, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.55G/15.1G [04:29<05:57, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.56G/15.1G [04:30<05:57, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.57G/15.1G [04:30<06:00, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  43% 6.59G/15.1G [04:31<06:01, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.60G/15.1G [04:31<05:58, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.61G/15.1G [04:32<05:57, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.62G/15.1G [04:32<05:56, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.63G/15.1G [04:33<05:55, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.64G/15.1G [04:33<05:54, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.65G/15.1G [04:33<05:52, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.66G/15.1G [04:34<05:50, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.67G/15.1G [04:34<05:50, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.68G/15.1G [04:35<05:51, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.69G/15.1G [04:35<05:49, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.70G/15.1G [04:36<05:50, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.71G/15.1G [04:36<05:49, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.72G/15.1G [04:36<05:48, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  44% 6.73G/15.1G [04:37<05:48, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.74G/15.1G [04:37<05:49, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.75G/15.1G [04:38<05:49, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.76G/15.1G [04:38<05:49, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.77G/15.1G [04:39<05:47, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.78G/15.1G [04:39<05:46, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.79G/15.1G [04:39<05:45, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.81G/15.1G [04:40<05:46, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.82G/15.1G [04:40<05:59, 23.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.83G/15.1G [04:41<05:42, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.84G/15.1G [04:41<05:45, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.85G/15.1G [04:42<05:45, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.86G/15.1G [04:42<05:44, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.87G/15.1G [04:43<05:44, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.88G/15.1G [04:43<05:43, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  45% 6.89G/15.1G [04:43<05:42, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.90G/15.1G [04:44<05:42, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.91G/15.1G [04:44<05:44, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.92G/15.1G [04:45<05:42, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.93G/15.1G [04:45<05:41, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.94G/15.1G [04:46<05:40, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.95G/15.1G [04:46<05:39, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.96G/15.1G [04:46<05:39, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.97G/15.1G [04:47<05:39, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.98G/15.1G [04:47<05:43, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 6.99G/15.1G [04:48<05:43, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 7.00G/15.1G [04:48<05:41, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 7.01G/15.1G [04:49<05:40, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 7.03G/15.1G [04:49<05:38, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  46% 7.04G/15.1G [04:50<05:37, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.05G/15.1G [04:50<05:38, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.06G/15.1G [04:50<05:40, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.07G/15.1G [04:51<05:37, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.08G/15.1G [04:51<05:36, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.09G/15.1G [04:52<05:34, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.10G/15.1G [04:52<05:33, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.11G/15.1G [04:53<05:35, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.12G/15.1G [04:53<05:33, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.13G/15.1G [04:53<05:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.14G/15.1G [04:54<05:31, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.15G/15.1G [04:54<05:30, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.16G/15.1G [04:55<05:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.17G/15.1G [04:55<05:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.18G/15.1G [04:56<05:30, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  47% 7.19G/15.1G [04:56<05:30, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.20G/15.1G [04:56<05:32, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.21G/15.1G [04:57<05:31, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.22G/15.1G [04:57<05:28, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.24G/15.1G [04:58<05:28, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.25G/15.1G [04:58<05:28, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.26G/15.1G [04:59<05:28, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.27G/15.1G [04:59<05:27, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.28G/15.1G [05:00<05:26, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.29G/15.1G [05:00<05:25, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.30G/15.1G [05:00<05:27, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.31G/15.1G [05:01<05:27, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.32G/15.1G [05:01<05:25, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.33G/15.1G [05:02<05:24, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  48% 7.34G/15.1G [05:02<05:23, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.35G/15.1G [05:03<05:23, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.36G/15.1G [05:03<05:27, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.37G/15.1G [05:03<05:28, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.38G/15.1G [05:04<05:26, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.39G/15.1G [05:04<05:23, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.40G/15.1G [05:05<05:22, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.41G/15.1G [05:05<05:23, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.42G/15.1G [05:06<05:21, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.43G/15.1G [05:06<05:21, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.44G/15.1G [05:07<05:20, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.46G/15.1G [05:07<05:19, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.47G/15.1G [05:07<05:18, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.48G/15.1G [05:08<05:18, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.49G/15.1G [05:08<05:17, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  49% 7.50G/15.1G [05:09<05:17, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.51G/15.1G [05:09<05:16, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.52G/15.1G [05:10<05:18, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.53G/15.1G [05:10<05:18, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.54G/15.1G [05:10<05:17, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.55G/15.1G [05:11<05:16, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.56G/15.1G [05:11<05:15, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.57G/15.1G [05:12<05:14, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.58G/15.1G [05:12<05:15, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.59G/15.1G [05:13<05:13, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.60G/15.1G [05:13<05:13, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.61G/15.1G [05:14<05:13, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.62G/15.1G [05:14<05:11, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.63G/15.1G [05:14<05:11, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  50% 7.64G/15.1G [05:15<05:11, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.65G/15.1G [05:15<05:11, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.67G/15.1G [05:16<05:14, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.68G/15.1G [05:16<05:13, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.69G/15.1G [05:17<05:12, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.70G/15.1G [05:17<05:10, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.71G/15.1G [05:17<05:09, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.72G/15.1G [05:18<05:08, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.73G/15.1G [05:18<05:13, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.74G/15.1G [05:19<05:13, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.75G/15.1G [05:19<05:10, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.76G/15.1G [05:20<05:08, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.77G/15.1G [05:20<05:07, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.78G/15.1G [05:21<05:06, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.79G/15.1G [05:21<05:05, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  51% 7.80G/15.1G [05:21<05:06, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.81G/15.1G [05:22<05:06, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.82G/15.1G [05:22<05:04, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.83G/15.1G [05:23<05:04, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.84G/15.1G [05:23<05:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.85G/15.1G [05:24<05:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.86G/15.1G [05:24<05:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.87G/15.1G [05:24<05:02, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.89G/15.1G [05:25<05:00, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.90G/15.1G [05:25<05:00, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.91G/15.1G [05:26<04:59, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.92G/15.1G [05:26<04:58, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.93G/15.1G [05:27<04:58, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.94G/15.1G [05:27<04:58, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  52% 7.95G/15.1G [05:28<05:10, 23.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 7.96G/15.1G [05:28<05:07, 23.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 7.97G/15.1G [05:28<05:03, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 7.98G/15.1G [05:29<05:00, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 7.99G/15.1G [05:29<04:58, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.00G/15.1G [05:30<04:56, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.01G/15.1G [05:30<04:57, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.02G/15.1G [05:31<04:56, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.03G/15.1G [05:31<04:55, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.04G/15.1G [05:31<04:54, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.05G/15.1G [05:32<04:53, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.06G/15.1G [05:32<04:52, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.07G/15.1G [05:33<04:53, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.08G/15.1G [05:33<04:52, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  53% 8.10G/15.1G [05:34<04:52, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.11G/15.1G [05:34<04:51, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.12G/15.1G [05:34<04:50, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.13G/15.1G [05:35<04:52, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.14G/15.1G [05:35<04:52, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.15G/15.1G [05:36<04:51, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.16G/15.1G [05:36<04:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.17G/15.1G [05:37<04:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.18G/15.1G [05:37<04:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.19G/15.1G [05:38<04:50, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.20G/15.1G [05:38<04:49, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.21G/15.1G [05:38<04:49, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.22G/15.1G [05:39<04:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.23G/15.1G [05:39<04:47, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.24G/15.1G [05:40<04:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  54% 8.25G/15.1G [05:40<04:47, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.26G/15.1G [05:41<04:46, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.27G/15.1G [05:41<04:46, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.28G/15.1G [05:41<04:50, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.29G/15.1G [05:42<04:53, 23.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.30G/15.1G [05:42<04:49, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.32G/15.1G [05:43<04:46, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.33G/15.1G [05:43<04:46, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.34G/15.1G [05:44<04:45, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.35G/15.1G [05:44<04:44, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.36G/15.1G [05:45<04:42, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.37G/15.1G [05:45<04:41, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.38G/15.1G [05:45<04:40, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.39G/15.1G [05:46<04:41, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  55% 8.40G/15.1G [05:46<04:40, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.41G/15.1G [05:47<04:39, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.42G/15.1G [05:47<04:38, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.43G/15.1G [05:48<04:37, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.44G/15.1G [05:48<04:37, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.45G/15.1G [05:48<04:36, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.46G/15.1G [05:49<04:36, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.47G/15.1G [05:49<04:36, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.48G/15.1G [05:50<04:37, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.49G/15.1G [05:50<04:35, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.50G/15.1G [05:51<04:34, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.51G/15.1G [05:51<04:35, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.52G/15.1G [05:52<04:36, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.54G/15.1G [05:52<04:34, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.55G/15.1G [05:52<04:34, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  56% 8.56G/15.1G [05:53<04:34, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.57G/15.1G [05:53<04:33, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.58G/15.1G [05:54<04:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.59G/15.1G [05:54<04:32, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.60G/15.1G [05:55<04:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.61G/15.1G [05:55<04:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.62G/15.1G [05:55<04:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.63G/15.1G [05:56<04:31, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.64G/15.1G [05:56<04:31, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.65G/15.1G [05:57<04:35, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.66G/15.1G [05:57<04:38, 23.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.67G/15.1G [05:58<04:37, 23.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.68G/15.1G [05:58<04:33, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.69G/15.1G [05:59<04:32, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  57% 8.70G/15.1G [05:59<04:29, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.71G/15.1G [05:59<04:28, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.72G/15.1G [06:00<04:27, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.73G/15.1G [06:00<04:26, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.75G/15.1G [06:01<04:25, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.76G/15.1G [06:01<04:25, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.77G/15.1G [06:02<04:24, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.78G/15.1G [06:02<04:23, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.79G/15.1G [06:02<04:23, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.80G/15.1G [06:03<04:23, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.81G/15.1G [06:03<04:22, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.82G/15.1G [06:04<04:22, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.83G/15.1G [06:04<04:21, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.84G/15.1G [06:05<04:23, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.85G/15.1G [06:05<04:22, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  58% 8.86G/15.1G [06:06<04:20, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.87G/15.1G [06:06<04:20, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.88G/15.1G [06:06<04:19, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.89G/15.1G [06:07<04:19, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.90G/15.1G [06:07<04:19, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.91G/15.1G [06:08<04:19, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.92G/15.1G [06:08<04:18, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.93G/15.1G [06:09<04:17, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.94G/15.1G [06:09<04:16, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.95G/15.1G [06:09<04:16, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.97G/15.1G [06:10<04:15, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.98G/15.1G [06:10<04:15, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 8.99G/15.1G [06:11<04:22, 23.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 9.00G/15.1G [06:11<04:23, 23.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  59% 9.01G/15.1G [06:12<04:22, 23.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.02G/15.1G [06:12<04:18, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.03G/15.1G [06:13<04:16, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.04G/15.1G [06:13<04:15, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.05G/15.1G [06:13<04:14, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.06G/15.1G [06:14<04:12, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.07G/15.1G [06:14<04:11, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.08G/15.1G [06:15<04:10, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.09G/15.1G [06:15<04:10, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.10G/15.1G [06:16<04:10, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.11G/15.1G [06:16<04:09, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.12G/15.1G [06:16<04:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.13G/15.1G [06:17<04:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.14G/15.1G [06:17<04:14, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.15G/15.1G [06:18<04:12, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  60% 9.16G/15.1G [06:18<04:12, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.18G/15.1G [06:19<04:10, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.19G/15.1G [06:19<04:08, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.20G/15.1G [06:20<04:08, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.21G/15.1G [06:20<04:07, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.22G/15.1G [06:20<04:06, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.23G/15.1G [06:21<04:05, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.24G/15.1G [06:21<04:04, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.25G/15.1G [06:22<04:03, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.26G/15.1G [06:22<04:03, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.27G/15.1G [06:23<04:04, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.28G/15.1G [06:23<04:03, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.29G/15.1G [06:23<04:03, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.30G/15.1G [06:24<04:01, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  61% 9.31G/15.1G [06:24<04:01, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.32G/15.1G [06:25<04:02, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.33G/15.1G [06:25<04:06, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.34G/15.1G [06:26<04:03, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.35G/15.1G [06:26<04:02, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.36G/15.1G [06:27<04:04, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.37G/15.1G [06:27<04:03, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.38G/15.1G [06:27<04:02, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.40G/15.1G [06:28<04:01, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.41G/15.1G [06:28<03:59, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.42G/15.1G [06:29<03:58, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.43G/15.1G [06:29<03:57, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.44G/15.1G [06:30<03:56, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.45G/15.1G [06:30<03:55, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  62% 9.46G/15.1G [06:30<03:56, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.47G/15.1G [06:31<03:55, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.48G/15.1G [06:31<03:54, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.49G/15.1G [06:32<03:58, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.50G/15.1G [06:32<03:57, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.51G/15.1G [06:33<03:56, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.52G/15.1G [06:33<03:54, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.53G/15.1G [06:33<03:53, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.54G/15.1G [06:34<03:53, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.55G/15.1G [06:34<03:52, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.56G/15.1G [06:35<03:51, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.57G/15.1G [06:35<03:50, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.58G/15.1G [06:36<03:50, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.59G/15.1G [06:36<03:50, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.60G/15.1G [06:37<03:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  63% 9.62G/15.1G [06:37<03:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.63G/15.1G [06:37<03:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.64G/15.1G [06:38<03:48, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.65G/15.1G [06:38<03:47, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.66G/15.1G [06:39<03:47, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.67G/15.1G [06:39<03:46, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.68G/15.1G [06:40<03:45, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.69G/15.1G [06:40<03:45, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.70G/15.1G [06:41<03:58, 22.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.71G/15.1G [06:41<03:40, 24.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.72G/15.1G [06:41<03:40, 24.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.73G/15.1G [06:42<03:41, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.74G/15.1G [06:42<03:46, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.75G/15.1G [06:43<03:48, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  64% 9.76G/15.1G [06:43<03:46, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.77G/15.1G [06:44<03:46, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.78G/15.1G [06:44<03:45, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.79G/15.1G [06:44<03:43, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.80G/15.1G [06:45<03:44, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.81G/15.1G [06:45<03:42, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.83G/15.1G [06:46<03:41, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.84G/15.1G [06:46<03:40, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.85G/15.1G [06:47<03:43, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.86G/15.1G [06:47<03:41, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.87G/15.1G [06:47<03:40, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.88G/15.1G [06:48<03:40, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.89G/15.1G [06:48<03:39, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.90G/15.1G [06:49<03:38, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.91G/15.1G [06:49<03:38, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  65% 9.92G/15.1G [06:50<03:36, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 9.93G/15.1G [06:50<03:35, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 9.94G/15.1G [06:51<03:36, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 9.95G/15.1G [06:51<03:45, 23.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 9.96G/15.1G [06:51<03:31, 24.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 9.97G/15.1G [06:52<03:32, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 9.98G/15.1G [06:52<03:34, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 9.99G/15.1G [06:53<03:33, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 10.0G/15.1G [06:53<03:32, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 10.0G/15.1G [06:54<03:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 10.0G/15.1G [06:54<03:32, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 10.0G/15.1G [06:54<03:31, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 10.0G/15.1G [06:55<03:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 10.1G/15.1G [06:55<03:30, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  66% 10.1G/15.1G [06:56<03:30, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.1G/15.1G [06:56<03:32, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.1G/15.1G [06:57<03:30, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.1G/15.1G [06:57<03:30, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.1G/15.1G [06:57<03:29, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.1G/15.1G [06:58<03:37, 23.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.1G/15.1G [06:58<03:26, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.1G/15.1G [06:59<03:26, 24.3MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.2G/15.1G [06:59<03:27, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.2G/15.1G [07:00<03:27, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.2G/15.1G [07:00<03:26, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.2G/15.1G [07:01<03:27, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.2G/15.1G [07:01<03:27, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.2G/15.1G [07:01<03:25, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.2G/15.1G [07:02<03:24, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  67% 10.2G/15.1G [07:02<03:25, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.2G/15.1G [07:03<03:25, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.2G/15.1G [07:03<03:25, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:04<03:24, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:04<03:23, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:05<03:24, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:05<03:23, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:05<03:21, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:06<03:21, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:06<03:20, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:07<03:20, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:07<03:19, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.3G/15.1G [07:08<03:18, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.4G/15.1G [07:08<03:18, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  68% 10.4G/15.1G [07:08<03:18, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.4G/15.1G [07:09<03:18, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.4G/15.1G [07:09<03:17, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.4G/15.1G [07:10<03:17, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.4G/15.1G [07:10<03:16, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.4G/15.1G [07:11<03:16, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.4G/15.1G [07:11<03:15, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.4G/15.1G [07:11<03:17, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.5G/15.1G [07:12<03:16, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.5G/15.1G [07:12<03:15, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.5G/15.1G [07:13<03:14, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.5G/15.1G [07:13<03:15, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.5G/15.1G [07:14<03:14, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.5G/15.1G [07:14<03:12, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.5G/15.1G [07:15<03:12, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  69% 10.5G/15.1G [07:15<03:11, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.5G/15.1G [07:15<03:10, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.5G/15.1G [07:16<03:10, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:16<03:09, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:17<03:09, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:17<03:09, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:18<03:09, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:18<03:11, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:18<03:10, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:19<03:08, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:19<03:07, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.6G/15.1G [07:20<03:07, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.7G/15.1G [07:20<03:06, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.7G/15.1G [07:21<03:06, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  70% 10.7G/15.1G [07:21<03:06, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.7G/15.1G [07:22<03:07, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.7G/15.1G [07:22<03:05, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.7G/15.1G [07:22<03:05, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.7G/15.1G [07:23<03:04, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.7G/15.1G [07:23<03:03, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.7G/15.1G [07:24<03:03, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.7G/15.1G [07:24<03:03, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.8G/15.1G [07:25<03:02, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.8G/15.1G [07:25<03:02, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.8G/15.1G [07:25<03:01, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.8G/15.1G [07:26<03:00, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.8G/15.1G [07:26<02:59, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.8G/15.1G [07:27<02:59, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  71% 10.8G/15.1G [07:27<02:59, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.8G/15.1G [07:28<02:59, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.8G/15.1G [07:28<03:00, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:28<02:59, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:29<02:59, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:29<02:58, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:30<02:57, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:30<02:57, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:31<02:56, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:31<02:56, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:32<02:56, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:32<02:56, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 10.9G/15.1G [07:32<02:54, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 11.0G/15.1G [07:33<02:54, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 11.0G/15.1G [07:33<02:55, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  72% 11.0G/15.1G [07:34<02:54, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.0G/15.1G [07:34<02:53, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.0G/15.1G [07:35<02:52, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.0G/15.1G [07:35<02:51, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.0G/15.1G [07:35<02:51, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.0G/15.1G [07:36<02:51, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.0G/15.1G [07:36<02:50, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.1G/15.1G [07:37<02:49, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.1G/15.1G [07:37<02:50, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.1G/15.1G [07:38<02:50, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.1G/15.1G [07:38<02:49, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.1G/15.1G [07:39<02:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.1G/15.1G [07:39<02:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.1G/15.1G [07:39<02:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  73% 11.1G/15.1G [07:40<02:47, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.1G/15.1G [07:40<02:46, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.1G/15.1G [07:41<02:45, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:41<02:45, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:42<02:44, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:42<02:44, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:42<02:44, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:43<02:44, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:43<02:45, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:44<02:45, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:44<02:44, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.2G/15.1G [07:45<02:43, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.3G/15.1G [07:45<02:42, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.3G/15.1G [07:46<02:42, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.3G/15.1G [07:46<02:41, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  74% 11.3G/15.1G [07:46<02:40, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.3G/15.1G [07:47<02:40, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.3G/15.1G [07:47<02:39, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.3G/15.1G [07:48<02:38, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.3G/15.1G [07:48<02:38, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.3G/15.1G [07:49<02:38, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.3G/15.1G [07:49<02:37, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.4G/15.1G [07:49<02:39, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.4G/15.1G [07:50<02:38, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.4G/15.1G [07:50<02:37, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.4G/15.1G [07:51<02:36, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.4G/15.1G [07:51<02:35, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.4G/15.1G [07:52<02:35, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.4G/15.1G [07:52<02:37, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  75% 11.4G/15.1G [07:53<02:36, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.4G/15.1G [07:53<02:35, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:53<02:34, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:54<02:33, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:54<02:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:55<02:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:55<02:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:56<02:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:56<02:30, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:56<02:30, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:57<02:29, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.5G/15.1G [07:57<02:29, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.6G/15.1G [07:58<02:29, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.6G/15.1G [07:58<02:28, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.6G/15.1G [07:59<02:27, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  76% 11.6G/15.1G [07:59<02:27, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.6G/15.1G [07:59<02:27, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.6G/15.1G [08:00<02:27, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.6G/15.1G [08:00<02:27, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.6G/15.1G [08:01<02:26, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.6G/15.1G [08:01<02:26, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.6G/15.1G [08:02<02:25, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.7G/15.1G [08:02<02:26, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.7G/15.1G [08:03<02:25, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.7G/15.1G [08:03<02:24, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.7G/15.1G [08:03<02:24, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.7G/15.1G [08:04<02:23, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.7G/15.1G [08:04<02:22, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.7G/15.1G [08:05<02:22, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  77% 11.7G/15.1G [08:05<02:21, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.7G/15.1G [08:06<02:21, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:06<02:20, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:06<02:19, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:07<02:19, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:07<02:19, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:08<02:19, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:08<02:18, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:09<02:20, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:09<02:20, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:10<02:18, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.8G/15.1G [08:10<02:17, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.9G/15.1G [08:10<02:16, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.9G/15.1G [08:11<02:16, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.9G/15.1G [08:11<02:16, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  78% 11.9G/15.1G [08:12<02:16, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 11.9G/15.1G [08:12<02:17, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 11.9G/15.1G [08:13<02:16, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 11.9G/15.1G [08:13<02:15, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 11.9G/15.1G [08:13<02:14, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 11.9G/15.1G [08:14<02:13, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:14<02:12, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:15<02:12, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:15<02:11, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:16<02:11, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:16<02:10, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:17<02:09, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:17<02:09, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:17<02:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  79% 12.0G/15.1G [08:18<02:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.0G/15.1G [08:18<02:08, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:19<02:07, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:19<02:07, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:20<02:07, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:20<02:08, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:20<02:08, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:21<02:10, 23.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:21<02:08, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:22<02:07, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.1G/15.1G [08:22<02:05, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.2G/15.1G [08:23<02:04, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.2G/15.1G [08:23<02:03, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.2G/15.1G [08:24<02:03, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  80% 12.2G/15.1G [08:24<02:02, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.2G/15.1G [08:24<02:02, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.2G/15.1G [08:25<02:01, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.2G/15.1G [08:25<02:01, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.2G/15.1G [08:26<02:01, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.2G/15.1G [08:26<02:01, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.2G/15.1G [08:27<02:00, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:27<02:00, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:27<01:59, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:28<01:58, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:28<01:58, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:29<01:58, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:29<01:58, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:30<01:58, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:30<01:57, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  81% 12.3G/15.1G [08:31<01:56, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:31<01:56, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:31<01:55, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:32<01:55, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:32<01:54, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:33<01:54, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:33<01:53, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:34<01:53, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:34<01:54, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:34<01:53, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.4G/15.1G [08:35<01:53, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.5G/15.1G [08:35<01:52, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.5G/15.1G [08:36<01:51, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.5G/15.1G [08:36<01:52, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  82% 12.5G/15.1G [08:37<01:51, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.5G/15.1G [08:37<01:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.5G/15.1G [08:38<01:49, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.5G/15.1G [08:38<01:49, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.5G/15.1G [08:38<01:49, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.5G/15.1G [08:39<01:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:39<01:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:40<01:47, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:40<01:47, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:41<01:46, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:41<01:46, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:41<01:45, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:42<01:45, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:42<01:44, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:43<01:43, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  83% 12.6G/15.1G [08:43<01:43, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:44<01:42, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:44<01:43, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:44<01:42, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:45<01:43, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:45<01:42, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:46<01:41, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:46<01:41, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:47<01:40, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.7G/15.1G [08:47<01:39, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.8G/15.1G [08:48<01:39, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.8G/15.1G [08:48<01:38, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.8G/15.1G [08:48<01:38, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.8G/15.1G [08:49<01:39, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  84% 12.8G/15.1G [08:49<01:38, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.8G/15.1G [08:50<01:38, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.8G/15.1G [08:50<01:38, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.8G/15.1G [08:51<01:39, 23.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.8G/15.1G [08:51<01:38, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.8G/15.1G [08:52<01:37, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:52<01:35, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:52<01:35, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:53<01:34, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:53<01:33, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:54<01:33, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:54<01:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:55<01:32, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:55<01:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:55<01:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  85% 12.9G/15.1G [08:56<01:31, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [08:56<01:30, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [08:57<01:30, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [08:57<01:29, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [08:58<01:29, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [08:58<01:29, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [08:58<01:28, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [08:59<01:28, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [08:59<01:27, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.0G/15.1G [09:00<01:27, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.1G/15.1G [09:00<01:27, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.1G/15.1G [09:01<01:26, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.1G/15.1G [09:01<01:26, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.1G/15.1G [09:02<01:25, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  86% 13.1G/15.1G [09:02<01:25, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.1G/15.1G [09:02<01:24, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.1G/15.1G [09:03<01:24, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.1G/15.1G [09:03<01:24, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.1G/15.1G [09:04<01:23, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.1G/15.1G [09:04<01:25, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:05<01:23, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:05<01:22, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:05<01:22, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:06<01:21, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:06<01:21, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:07<01:21, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:07<01:20, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:08<01:19, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.2G/15.1G [09:08<01:19, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  87% 13.3G/15.1G [09:09<01:18, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:09<01:18, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:09<01:17, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:10<01:17, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:10<01:16, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:11<01:16, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:11<01:15, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:12<01:15, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:12<01:14, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.3G/15.1G [09:12<01:14, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.4G/15.1G [09:13<01:14, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.4G/15.1G [09:13<01:13, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.4G/15.1G [09:14<01:13, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.4G/15.1G [09:14<01:12, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  88% 13.4G/15.1G [09:15<01:12, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.4G/15.1G [09:15<01:12, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.4G/15.1G [09:15<01:11, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.4G/15.1G [09:16<01:11, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.4G/15.1G [09:16<01:11, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:17<01:10, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:17<01:10, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:18<01:09, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:18<01:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:19<01:08, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:19<01:08, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:19<01:09, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:20<01:08, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:20<01:07, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  89% 13.5G/15.1G [09:21<01:06, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:21<01:06, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:22<01:05, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:22<01:06, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:23<01:05, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:23<01:05, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:23<01:04, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:24<01:04, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:24<01:03, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.6G/15.1G [09:25<01:02, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.7G/15.1G [09:25<01:02, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.7G/15.1G [09:26<01:01, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.7G/15.1G [09:26<01:01, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.7G/15.1G [09:26<01:00, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.7G/15.1G [09:27<01:00, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  90% 13.7G/15.1G [09:27<00:59, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.7G/15.1G [09:28<00:59, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.7G/15.1G [09:28<00:58, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.7G/15.1G [09:29<00:58, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.7G/15.1G [09:29<00:57, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:29<00:57, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:30<00:57, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:30<00:56, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:31<00:56, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:31<00:55, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:32<00:55, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:32<00:55, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:33<00:55, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.8G/15.1G [09:33<00:54, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  91% 13.9G/15.1G [09:33<00:53, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:34<00:53, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:34<00:52, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:35<00:53, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:35<00:53, 23.4MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:36<00:52, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:36<00:51, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:36<00:51, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:37<00:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 13.9G/15.1G [09:37<00:50, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 14.0G/15.1G [09:38<00:49, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 14.0G/15.1G [09:38<00:49, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 14.0G/15.1G [09:39<00:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 14.0G/15.1G [09:39<00:48, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 14.0G/15.1G [09:40<00:47, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  92% 14.0G/15.1G [09:40<00:47, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.0G/15.1G [09:40<00:46, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.0G/15.1G [09:41<00:46, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.0G/15.1G [09:41<00:46, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:42<00:45, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:42<00:45, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:43<00:44, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:43<00:44, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:43<00:43, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:44<00:43, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:44<00:42, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:45<00:42, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:45<00:42, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.1G/15.1G [09:46<00:41, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  93% 14.2G/15.1G [09:46<00:41, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.2G/15.1G [09:47<00:40, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.2G/15.1G [09:47<00:40, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.2G/15.1G [09:47<00:40, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.2G/15.1G [09:48<00:39, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.2G/15.1G [09:48<00:39, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.2G/15.1G [09:49<00:38, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.2G/15.1G [09:49<00:38, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.2G/15.1G [09:50<00:37, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.3G/15.1G [09:50<00:37, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.3G/15.1G [09:50<00:37, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.3G/15.1G [09:51<00:37, 23.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.3G/15.1G [09:51<00:36, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.3G/15.1G [09:52<00:36, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.3G/15.1G [09:52<00:35, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  94% 14.3G/15.1G [09:53<00:35, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.3G/15.1G [09:53<00:34, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.3G/15.1G [09:54<00:33, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.3G/15.1G [09:54<00:33, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:54<00:32, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:55<00:32, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:55<00:32, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:56<00:31, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:56<00:31, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:57<00:30, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:57<00:30, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:57<00:29, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:58<00:29, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.4G/15.1G [09:58<00:29, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  95% 14.5G/15.1G [09:59<00:28, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.5G/15.1G [09:59<00:28, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.5G/15.1G [10:00<00:27, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.5G/15.1G [10:00<00:27, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.5G/15.1G [10:01<00:26, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.5G/15.1G [10:01<00:26, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.5G/15.1G [10:01<00:25, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.5G/15.1G [10:02<00:25, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.5G/15.1G [10:02<00:25, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.6G/15.1G [10:03<00:24, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.6G/15.1G [10:03<00:24, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.6G/15.1G [10:04<00:23, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.6G/15.1G [10:04<00:23, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.6G/15.1G [10:04<00:22, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.6G/15.1G [10:05<00:22, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  96% 14.6G/15.1G [10:05<00:22, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.6G/15.1G [10:06<00:22, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.6G/15.1G [10:06<00:21, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.6G/15.1G [10:07<00:20, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:07<00:20, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:07<00:19, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:08<00:19, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:08<00:19, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:09<00:18, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:09<00:18, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:10<00:17, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:10<00:17, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.7G/15.1G [10:11<00:16, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.8G/15.1G [10:11<00:16, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  97% 14.8G/15.1G [10:11<00:15, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.8G/15.1G [10:12<00:15, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.8G/15.1G [10:12<00:15, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.8G/15.1G [10:13<00:14, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.8G/15.1G [10:13<00:14, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.8G/15.1G [10:14<00:13, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.8G/15.1G [10:14<00:13, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.8G/15.1G [10:14<00:12, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.8G/15.1G [10:15<00:12, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.9G/15.1G [10:15<00:11, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.9G/15.1G [10:16<00:11, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.9G/15.1G [10:16<00:11, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.9G/15.1G [10:17<00:10, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.9G/15.1G [10:17<00:10, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.9G/15.1G [10:18<00:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  98% 14.9G/15.1G [10:18<00:09, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 14.9G/15.1G [10:18<00:08, 24.2MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 14.9G/15.1G [10:19<00:08, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:19<00:08, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:20<00:07, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:20<00:07, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:21<00:06, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:21<00:06, 23.7MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:21<00:06, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:22<00:05, 23.5MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:22<00:05, 23.6MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:23<00:04, 23.8MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.0G/15.1G [10:23<00:04, 23.9MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.1G/15.1G [10:24<00:03, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf:  99% 15.1G/15.1G [10:24<00:03, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf: 100% 15.1G/15.1G [10:25<00:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf: 100% 15.1G/15.1G [10:25<00:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf: 100% 15.1G/15.1G [10:25<00:02, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf: 100% 15.1G/15.1G [10:26<00:01, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf: 100% 15.1G/15.1G [10:26<00:01, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf: 100% 15.1G/15.1G [10:27<00:00, 24.0MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf: 100% 15.1G/15.1G [10:27<00:00, 24.1MB/s]\u001b[A\n",
            "llama-7b-relu.powerinfer.gguf: 100% 15.1G/15.1G [10:27<00:00, 24.1MB/s]\n",
            "Download complete. Moving file to ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf\n",
            "Fetching 37 files: 100% 37/37 [10:30<00:00, 17.04s/it]\n",
            "/content/PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ./build/bin/main -m ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf -n 200 -t 8 -p \"UIUC is a university\"\n",
        "# e.g.: ./build/bin/main -m ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf -n 128 -t 8 -p \"Once upon a time\"\n",
        "# For Windows: .\\build\\bin\\Release\\main.exe -m .\\ReluFalcon-40B-PowerInfer-GGUF\\falcon-40b-relu.q4.powerinfer.gguf -n 128 -t 8 -p \"Once upon a time\""
      ],
      "metadata": {
        "id": "gQlw7O0q1yDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "aa5cdde2-7ab8-4e86-ce15-ee32a349d7ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 1583 (61cac9b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1722803169\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5\n",
            "llama_model_loader: loaded meta data with 18 key-value pairs and 355 tensors from ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
            "llama_model_loader: - tensor    1:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    2:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    3:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    4:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor    7:          blk.0.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   10:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   11:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   12:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   13:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   16:          blk.1.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   19:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   20:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   21:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   22:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   25:          blk.2.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   28:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   29:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   30:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   31:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   34:          blk.3.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   37:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   38:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   39:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   40:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   43:          blk.4.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   46:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   47:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   48:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   49:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   52:          blk.5.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   55:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   56:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   57:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   58:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   61:          blk.6.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   64:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   65:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   66:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   67:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   70:          blk.7.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   73:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   74:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   75:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   76:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   79:          blk.8.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   82:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   83:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   84:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   85:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   88:          blk.9.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   91:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   92:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   93:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   94:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   97:         blk.10.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  100:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  101:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  102:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  103:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  106:         blk.11.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  109:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  110:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  111:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  112:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  115:         blk.12.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  118:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  119:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  120:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  121:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  124:         blk.13.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  127:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  128:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  129:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  130:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  133:         blk.14.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  136:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  137:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  138:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  139:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  142:         blk.15.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  145:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  146:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  147:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  148:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  151:         blk.16.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  154:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  155:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  156:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  157:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  160:         blk.17.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  163:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  164:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  165:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  166:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  169:         blk.18.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  172:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  173:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  174:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  175:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  178:         blk.19.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  181:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  182:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  183:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  184:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  187:         blk.20.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  190:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  191:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  192:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  193:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  196:         blk.21.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  199:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  200:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  201:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  202:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  205:         blk.22.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  208:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  209:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  210:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  211:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  214:         blk.23.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  217:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  218:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  219:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  220:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  223:         blk.24.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  226:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  227:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  228:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  229:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  232:         blk.25.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  235:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  236:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  237:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  238:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  241:         blk.26.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  244:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  245:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  246:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  247:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  250:         blk.27.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  253:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  254:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  255:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  256:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  259:         blk.28.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  262:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  263:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  264:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  265:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  268:         blk.29.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  271:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  272:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  273:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  274:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  277:         blk.30.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  280:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  281:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  282:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  283:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  286:         blk.31.ffn_down_t.weight f16      [  4096, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  290:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
            "llama_model_loader: - tensor  291:                 blk.0.fc1.weight f16      [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  292:                 blk.0.fc2.weight f16      [  1024, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  293:                 blk.1.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  294:                 blk.1.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  295:                 blk.2.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  296:                 blk.2.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  297:                 blk.3.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  298:                 blk.3.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  299:                 blk.4.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  300:                 blk.4.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  301:                 blk.5.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  302:                 blk.5.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  303:                 blk.6.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  304:                 blk.6.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  305:                 blk.7.fc1.weight f16      [  4096,  1536,     1,     1 ]\n",
            "llama_model_loader: - tensor  306:                 blk.7.fc2.weight f16      [  1536, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  307:                 blk.8.fc1.weight f16      [  4096,  1536,     1,     1 ]\n",
            "llama_model_loader: - tensor  308:                 blk.8.fc2.weight f16      [  1536, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  309:                 blk.9.fc1.weight f16      [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  310:                 blk.9.fc2.weight f16      [  1024, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  311:                blk.10.fc1.weight f16      [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  312:                blk.10.fc2.weight f16      [  1024, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  313:                blk.11.fc1.weight f16      [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  314:                blk.11.fc2.weight f16      [  1024, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  315:                blk.12.fc1.weight f16      [  4096,  1280,     1,     1 ]\n",
            "llama_model_loader: - tensor  316:                blk.12.fc2.weight f16      [  1280, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  317:                blk.13.fc1.weight f16      [  4096,  1280,     1,     1 ]\n",
            "llama_model_loader: - tensor  318:                blk.13.fc2.weight f16      [  1280, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  319:                blk.14.fc1.weight f16      [  4096,  1536,     1,     1 ]\n",
            "llama_model_loader: - tensor  320:                blk.14.fc2.weight f16      [  1536, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  321:                blk.15.fc1.weight f16      [  4096,  1536,     1,     1 ]\n",
            "llama_model_loader: - tensor  322:                blk.15.fc2.weight f16      [  1536, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  323:                blk.16.fc1.weight f16      [  4096,  1536,     1,     1 ]\n",
            "llama_model_loader: - tensor  324:                blk.16.fc2.weight f16      [  1536, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  325:                blk.17.fc1.weight f16      [  4096,  1536,     1,     1 ]\n",
            "llama_model_loader: - tensor  326:                blk.17.fc2.weight f16      [  1536, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  327:                blk.18.fc1.weight f16      [  4096,  1536,     1,     1 ]\n",
            "llama_model_loader: - tensor  328:                blk.18.fc2.weight f16      [  1536, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  329:                blk.19.fc1.weight f16      [  4096,  1792,     1,     1 ]\n",
            "llama_model_loader: - tensor  330:                blk.19.fc2.weight f16      [  1792, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  331:                blk.20.fc1.weight f16      [  4096,  1792,     1,     1 ]\n",
            "llama_model_loader: - tensor  332:                blk.20.fc2.weight f16      [  1792, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  333:                blk.21.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  334:                blk.21.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  335:                blk.22.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  336:                blk.22.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  337:                blk.23.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  338:                blk.23.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  339:                blk.24.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  340:                blk.24.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  341:                blk.25.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  342:                blk.25.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  343:                blk.26.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  344:                blk.26.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  345:                blk.27.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  346:                blk.27.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  347:                blk.28.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  348:                blk.28.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  349:                blk.29.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  350:                blk.29.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  351:                blk.30.fc1.weight f16      [  4096,  2048,     1,     1 ]\n",
            "llama_model_loader: - tensor  352:                blk.30.fc2.weight f16      [  2048, 11008,     1,     1 ]\n",
            "llama_model_loader: - tensor  353:                blk.31.fc1.weight f16      [  4096,  1536,     1,     1 ]\n",
            "llama_model_loader: - tensor  354:                blk.31.fc2.weight f16      [  1536, 11008,     1,     1 ]\n",
            "llama_model_loader: - kv   0:                       general.architecture str     \n",
            "llama_model_loader: - kv   1:                               general.name str     \n",
            "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
            "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
            "llama_model_loader: - kv  10:                          general.file_type u32     \n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32     \n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  290 tensors\n",
            "llama_model_load: PowerInfer model loaded. Sparse inference will be used.\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = mostly F16\n",
            "llm_load_print_meta: model params     = 7.57 B\n",
            "llm_load_print_meta: model size       = 14.11 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name   = syx\n",
            "llm_load_print_meta: BOS token = 1 '<s>'\n",
            "llm_load_print_meta: EOS token = 2 '</s>'\n",
            "llm_load_print_meta: UNK token = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token = 0 '<unk>'\n",
            "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
            "llm_load_print_meta: sparse_pred_threshold = 0.00\n",
            "llm_load_sparse_model_tensors: ggml ctx size =    0.13 MB\n",
            "llm_load_sparse_model_tensors: using CUDA for GPU acceleration\n",
            "llm_load_sparse_model_tensors: offloaded layers from VRAM budget(15446638592 bytes): 33/32\n",
            "llm_load_sparse_model_tensors: mem required  = 14446.15 MB\n",
            "llm_load_sparse_model_tensors: VRAM used: 5940.02 MB\n",
            "....................................................................................................\n",
            "invoking powerinfer Python module to generate gpu split for 8535.05 MiB of VRAM\n",
            "solver args: Namespace(activation='./ReluLLaMA-7B-PowerInfer-GGUF/activation', neuron=11008, capacity=361336, layer=32, vram_capacity=8949645312, batch=256, threshold=0, output='./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf.generated.gpuidx')\n",
            "GLPK Integer Optimizer 5.0\n",
            "65 rows, 1408 columns, 4160 non-zeros\n",
            "1408 integer variables, all of which are binary\n",
            "Preprocessing...\n",
            "32 constraint coefficient(s) were reduced\n",
            "32 rows, 1408 columns, 1408 non-zeros\n",
            "1408 integer variables, all of which are binary\n",
            "Scaling...\n",
            " A: min|aij| =  1.000e+00  max|aij| =  4.300e+01  ratio =  4.300e+01\n",
            "GM: min|aij| =  1.000e+00  max|aij| =  1.000e+00  ratio =  1.000e+00\n",
            "EQ: min|aij| =  1.000e+00  max|aij| =  1.000e+00  ratio =  1.000e+00\n",
            "2N: min|aij| =  6.719e-01  max|aij| =  1.000e+00  ratio =  1.488e+00\n",
            "Constructing initial basis...\n",
            "Size of triangular part is 32\n",
            "Solving LP relaxation...\n",
            "GLPK Simplex Optimizer 5.0\n",
            "32 rows, 1408 columns, 1408 non-zeros\n",
            "*     0: obj =   0.000000000e+00 inf =   0.000e+00 (1376)\n",
            "*  1408: obj =  -1.293543558e+10 inf =   0.000e+00 (0)\n",
            "OPTIMAL LP SOLUTION FOUND\n",
            "Integer optimization begins...\n",
            "Long-step dual simplex will be used\n",
            "+  1408: mip =     not found yet >=              -inf        (1; 0)\n",
            "+  1408: >>>>>  -1.293543558e+10 >=  -1.293543558e+10   0.0% (1; 0)\n",
            "+  1408: mip =  -1.293543558e+10 >=     tree is empty   0.0% (0; 1)\n",
            "INTEGER OPTIMAL SOLUTION FOUND\n",
            "ILP Status: optimal\n",
            "Total Activation Units: 1408.0\n",
            "solved: [11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0, 11008.0], total neurons: 352256.0\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "blk.0.gpu_idx => blk.0.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.0.gpu_bucket => blk.0.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.1.gpu_idx => blk.1.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.1.gpu_bucket => blk.1.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.2.gpu_idx => blk.2.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.2.gpu_bucket => blk.2.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.3.gpu_idx => blk.3.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.3.gpu_bucket => blk.3.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.4.gpu_idx => blk.4.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.4.gpu_bucket => blk.4.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.5.gpu_idx => blk.5.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.5.gpu_bucket => blk.5.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.6.gpu_idx => blk.6.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.6.gpu_bucket => blk.6.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.7.gpu_idx => blk.7.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.7.gpu_bucket => blk.7.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.8.gpu_idx => blk.8.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.8.gpu_bucket => blk.8.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.9.gpu_idx => blk.9.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.9.gpu_bucket => blk.9.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.10.gpu_idx => blk.10.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.10.gpu_bucket => blk.10.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.11.gpu_idx => blk.11.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.11.gpu_bucket => blk.11.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.12.gpu_idx => blk.12.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.12.gpu_bucket => blk.12.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.13.gpu_idx => blk.13.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.13.gpu_bucket => blk.13.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.14.gpu_idx => blk.14.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.14.gpu_bucket => blk.14.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.15.gpu_idx => blk.15.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.15.gpu_bucket => blk.15.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.16.gpu_idx => blk.16.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.16.gpu_bucket => blk.16.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.17.gpu_idx => blk.17.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.17.gpu_bucket => blk.17.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.18.gpu_idx => blk.18.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.18.gpu_bucket => blk.18.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.19.gpu_idx => blk.19.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.19.gpu_bucket => blk.19.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.20.gpu_idx => blk.20.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.20.gpu_bucket => blk.20.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.21.gpu_idx => blk.21.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.21.gpu_bucket => blk.21.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.22.gpu_idx => blk.22.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.22.gpu_bucket => blk.22.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.23.gpu_idx => blk.23.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.23.gpu_bucket => blk.23.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.24.gpu_idx => blk.24.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.24.gpu_bucket => blk.24.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.25.gpu_idx => blk.25.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.25.gpu_bucket => blk.25.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.26.gpu_idx => blk.26.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.26.gpu_bucket => blk.26.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.27.gpu_idx => blk.27.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.27.gpu_bucket => blk.27.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.28.gpu_idx => blk.28.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.28.gpu_bucket => blk.28.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.29.gpu_idx => blk.29.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.29.gpu_bucket => blk.29.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.30.gpu_idx => blk.30.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.30.gpu_bucket => blk.30.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "blk.31.gpu_idx => blk.31.gpu_idx (11008,) int32 0.0419921875 MiB\n",
            "blk.31.gpu_bucket => blk.31.gpu_bucket (11008,) int32 0.0419921875 MiB\n",
            "exported GPU index to ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf.generated.gpuidx\n",
            "Exported to ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf.generated.gpuidx\n",
            "llama_model_loader: loaded meta data with 3 key-value pairs and 64 tensors from ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf.generated.gpuidx (version GGUF V3 (latest))\n",
            "llama_model_loader: - tensor    0:                    blk.0.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    1:                 blk.0.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    2:                    blk.1.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    3:                 blk.1.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    4:                    blk.2.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    5:                 blk.2.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    6:                    blk.3.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    7:                 blk.3.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    8:                    blk.4.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    9:                 blk.4.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   10:                    blk.5.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   11:                 blk.5.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   12:                    blk.6.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   13:                 blk.6.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   14:                    blk.7.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   15:                 blk.7.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   16:                    blk.8.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   17:                 blk.8.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   18:                    blk.9.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   19:                 blk.9.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   20:                   blk.10.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   21:                blk.10.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   22:                   blk.11.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   23:                blk.11.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   24:                   blk.12.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   25:                blk.12.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   26:                   blk.13.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   27:                blk.13.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   28:                   blk.14.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   29:                blk.14.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   30:                   blk.15.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   31:                blk.15.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   32:                   blk.16.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   33:                blk.16.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   34:                   blk.17.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   35:                blk.17.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   36:                   blk.18.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   37:                blk.18.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   38:                   blk.19.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   39:                blk.19.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   40:                   blk.20.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   41:                blk.20.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   42:                   blk.21.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   43:                blk.21.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   44:                   blk.22.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   45:                blk.22.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   46:                   blk.23.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   47:                blk.23.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   48:                   blk.24.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   49:                blk.24.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   50:                   blk.25.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   51:                blk.25.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   52:                   blk.26.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   53:                blk.26.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   54:                   blk.27.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   55:                blk.27.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   56:                   blk.28.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   57:                blk.28.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   58:                   blk.29.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   59:                blk.29.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   60:                   blk.30.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   61:                blk.30.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   62:                   blk.31.gpu_idx i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   63:                blk.31.gpu_bucket i32      [ 11008,     1,     1,     1 ]\n",
            "llama_model_loader: unknown type i32\n",
            "llama_model_loader: - kv   0:                       general.architecture str     \n",
            "llama_model_loader: - kv   1:              generic.gpu_index.block_count u32     \n",
            "llama_model_loader: - kv   2:                        split.vram_capacity u64     \n",
            "llama_model_loader: - type  i32:   64 tensors\n",
            "loaded gpu_idx, vram_required: 8949645312\n",
            "load_gpu_idx_for_model: applying gpu_idx adapter from './ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf.generated.gpuidx' - please wait ...\n",
            "................................................................ done (1.92 ms)\n",
            "offload_ffn_split: applying augmentation to model - please wait ...\n",
            "................................ done (21435.43 ms)\n",
            "llm_load_gpu_split: offloaded 8256.00 MiB of FFN weights to GPU\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init: offloading v cache to GPU\n",
            "llama_kv_cache_init: offloading k cache to GPU\n",
            "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
            "llama_new_context_with_model: kv self size  =  256.00 MB\n",
            "llama_build_graph: non-view tensors processed: 580/836\n",
            "llama_build_graph: ****************************************************************\n",
            "llama_build_graph: not all non-view tensors have been processed with a callback\n",
            "llama_build_graph: this can indicate an inefficiency in the graph implementation\n",
            "llama_build_graph: build with LLAMA_OFFLOAD_DEBUG for more info\n",
            "llama_build_graph: ref: https://github.com/ggerganov/llama.cpp/pull/3837\n",
            "llama_build_graph: ****************************************************************\n",
            "llama_new_context_with_model: compute buffer total size = 87.07 MB\n",
            "llama_new_context_with_model: VRAM scratch buffer: 85.50 MB\n",
            "llama_new_context_with_model: total VRAM used: 22793.52 MB (model: 14196.02 MB, context: 341.50 MB)\n",
            "\n",
            "system_info: n_threads = 8 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 200, n_keep = 0\n",
            "\n",
            "\n",
            "UIUC is a university with a strong international reputation for research, teaching and learning in all its three missions of education, research, service. This is reflected in its ranking as one of the top ten universities in the world by many international rankings. The university has nine colleges: Arts & Sciences; Business; College of Law; College of Liberal Arts and Sciences; College of Engineering; College of Medicine; College of Agricultural, Consumer and Environmental Sciences; College of Education and College on Health; and the UIUC Research Park.\n",
            " [end of text]\n",
            "\n",
            "llama_print_timings:        load time =  106736.24 ms\n",
            "llama_print_timings:      sample time =      59.57 ms /   108 runs   (    0.55 ms per token,  1812.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8796.59 ms /     6 tokens ( 1466.10 ms per token,     0.68 tokens per second)\n",
            "llama_print_timings:        eval time =  936296.49 ms /   107 runs   ( 8750.43 ms per token,     0.11 tokens per second)\n",
            "llama_print_timings:       total time =  945196.29 ms\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Znye-wPEVpo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}